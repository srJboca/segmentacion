{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Modelo de Predicción de Mora con PCA\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Este notebook demuestra el proceso de construcción de un modelo de predicción para la variable 'Mora' (incumplimiento de pago). Un aspecto clave de este tutorial es el uso del Análisis de Componentes Principales (PCA) para la reducción de dimensionalidad antes de entrenar un modelo RandomForestClassifier. Además, el notebook ilustra un proceso iterativo de modelado, donde se identifica y corrige un problema común de fuga de datos (data leakage), lo cual es una lección importante en la práctica de la ciencia de datos.\n",
        "\n",
        "**Pasos del Tutorial:**\n",
        "1.  Configuración inicial y carga de datos.\n",
        "2.  Preparación de datos y selección inicial de características.\n",
        "3.  Primer intento de modelado con PCA, que revelará un problema de fuga de datos.\n",
        "4.  Segundo intento de modelado, corrigiendo la fuga de datos y aplicando PCA nuevamente.\n",
        "5.  Estimación de la importancia de las características originales después de la aplicación de PCA.\n",
        "\n",
        "A través de estos pasos, veremos cómo se puede refinar un modelo y la importancia de una cuidadosa selección y preparación de características."
      ],
      "metadata": {
        "id": "intro_pred_pca_tutorial_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuración Inicial y Carga de Datos"
      ],
      "metadata": {
        "id": "setup_and_load_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Importación de Librerías Necesarias\n",
        "Comenzamos importando las librerías que se utilizarán para la manipulación de datos, visualización, preprocesamiento y modelado."
      ],
      "metadata": {
        "id": "import_libs_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "ldvwepAOlLTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Carga del Dataset\n",
        "Descargamos y cargamos el dataset `df_analisis.parquet`, que contiene los datos preprocesados de facturación y clientes de un notebook anterior."
      ],
      "metadata": {
        "id": "load_data_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/df_analisis.parquet\n",
        "df_analisis = pd.read_parquet('df_analisis.parquet')"
      ],
      "metadata": {
        "id": "AuyjTblClMGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preparación de Datos para la Predicción\n",
        "\n",
        "El título original de esta sección era \"# Exploración de los datos\", pero dado el contenido, se enfoca más en la preparación para la predicción."
      ],
      "metadata": {
        "id": "data_prep_title_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_analisis = pd.read_parquet('df_analisis.parquet') # Ya cargado arriba\n",
        "# Visualización inicial para recordar la estructura\n",
        "print(\"--- Primeras filas de df_analisis ---\")\n",
        "print(df_analisis.head())\n",
        "df_analisis.info()"
      ],
      "metadata": {
        "id": "9S6RHKsclJWn_recheck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Selección Inicial de Características\n",
        "El notebook original titula esta sección \"# Ejercicios de prediccion con PCA\".\n",
        "Se selecciona un subconjunto de columnas de `df_analisis` para crear `df_prediccion`. Es importante notar que en esta selección inicial se incluye `Dias_PagoOportuno_PagoReal`, una variable que está directamente relacionada con la definición de `Mora`."
      ],
      "metadata": {
        "id": "feature_selection_markdown_cell5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prediccion = df_analisis[['Numero de factura', 'Consumo (m3)', 'Estrato', 'Precio por Consumo', 'Dias_Emision_PagoOportuno', 'Dias_Lectura_Emision', 'Dias_PagoOportuno_PagoReal', 'Mora']]\n",
        "print(\"--- df_prediccion (primeras filas después de selección inicial) ---\")\n",
        "print(df_prediccion.head())"
      ],
      "metadata": {
        "id": "JuXAGsbYalt9_and_U9MGsGQra3Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Preprocesamiento Básico de Características\n",
        "A continuación, se realizan varias tareas de preprocesamiento:\n",
        "- Se elimina la columna 'Numero de factura', ya que es un identificador y no una característica predictiva.\n",
        "- La columna 'Estrato' se convierte de un formato de texto (ej. \"Estrato 1\") a un valor numérico entero.\n",
        "- Los valores faltantes (NaN) en el DataFrame se rellenan con la media de sus respectivas columnas."
      ],
      "metadata": {
        "id": "basic_preprocessing_markdown_cell7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prediccion_ml = df_prediccion.drop('Numero de factura', axis=1).copy() # Se usa .copy() para evitar SettingWithCopyWarning\n",
        "\n",
        "# Convertir 'Estrato' a numérico ordinal\n",
        "if df_prediccion_ml['Estrato'].dtype == 'object' or isinstance(df_prediccion_ml['Estrato'].dtype, pd.CategoricalDtype):\n",
        "    df_prediccion_ml['Estrato'] = df_prediccion_ml['Estrato'].str.replace('Estrato ', '', regex=False).astype(int)\n",
        "else:\n",
        "    df_prediccion_ml['Estrato'] = df_prediccion_ml['Estrato'].astype(int)\n",
        "\n",
        "# Imputar NaNs con la media de cada columna\n",
        "for col in df_prediccion_ml.columns:\n",
        "    if df_prediccion_ml[col].isnull().any():\n",
        "        df_prediccion_ml[col].fillna(df_prediccion_ml[col].mean(), inplace=True)\n",
        "\n",
        "print(\"--- df_prediccion_ml después de preprocesamiento básico (primeras filas) ---\")\n",
        "print(df_prediccion_ml.head())\n",
        "print(\"\\n--- Verificación de NaNs restantes ---\")\n",
        "print(df_prediccion_ml.isnull().sum())"
      ],
      "metadata": {
        "id": "cell7_code_content"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modelado con PCA - Primer Intento (Ilustrando Fuga de Datos)\n",
        "\n",
        "En este primer intento, construiremos un modelo utilizando las características preparadas, incluyendo `Dias_PagoOportuno_PagoReal`. Como se mencionó, esto probablemente resultará en una fuga de datos."
      ],
      "metadata": {
        "id": "modeling1_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Definición de Características (X) y Variable Objetivo (y)"
      ],
      "metadata": {
        "id": "define_Xy_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_leak = df_prediccion_ml.drop('Mora', axis=1)\n",
        "y_leak = df_prediccion_ml['Mora']\n",
        "\n",
        "print(\"--- Características X_leak (primeras filas) ---\")\n",
        "print(X_leak.head())\n",
        "print(\"\\n--- Variable objetivo y_leak (primeras filas) ---\")\n",
        "print(y_leak.head())"
      ],
      "metadata": {
        "id": "define_Xy_leak_code_part_of_cell7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Escalado de Características\n",
        "Antes de aplicar PCA, es crucial escalar las características para que todas tengan una media de 0 y una desviación estándar de 1. Esto asegura que las características con magnitudes mayores no dominen el análisis de componentes principales."
      ],
      "metadata": {
        "id": "scaling_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_leak = StandardScaler()\n",
        "X_scaled_leak = scaler_leak.fit_transform(X_leak)\n",
        "\n",
        "print(\"--- X_scaled_leak (primeras 5 filas de características escaladas) ---\")\n",
        "print(pd.DataFrame(X_scaled_leak, columns=X_leak.columns).head())"
      ],
      "metadata": {
        "id": "cell8_code_content"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Aplicación de PCA\n",
        "Aplicamos PCA para reducir la dimensionalidad del conjunto de datos. Se configura `n_components=0.95`, lo que significa que PCA seleccionará el número mínimo de componentes principales que logren explicar al menos el 95% de la varianza en los datos escalados."
      ],
      "metadata": {
        "id": "pca_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_leak = PCA(n_components=0.95)\n",
        "X_pca_leak = pca_leak.fit_transform(X_scaled_leak)\n",
        "\n",
        "print(f\"Número de componentes seleccionados por PCA (con leakage): {pca_leak.n_components_}\")\n",
        "print(f\"Varianza explicada por cada componente: {pca_leak.explained_variance_ratio_}\")\n",
        "print(f\"Varianza explicada acumulada: {pca_leak.explained_variance_ratio_.sum():.4f}\")\n",
        "print(\"\\n--- X_pca_leak (primeras 5 filas de componentes principales) ---\")\n",
        "print(pd.DataFrame(X_pca_leak).head())"
      ],
      "metadata": {
        "id": "cell9_code_content"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 División en Conjuntos de Entrenamiento y Prueba"
      ],
      "metadata": {
        "id": "split_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pca_leak, X_test_pca_leak, y_train_leak, y_test_leak = train_test_split(\n",
        "    X_pca_leak, y_leak, test_size=0.2, random_state=42, stratify=y_leak\n",
        ")\n",
        "\n",
        "print(f\"Dimensiones de X_train_pca_leak: {X_train_pca_leak.shape}\")\n",
        "print(f\"Dimensiones de X_test_pca_leak: {X_test_pca_leak.shape}\")"
      ],
      "metadata": {
        "id": "cell10_code_content"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Entrenamiento y Evaluación del Modelo RandomForest (Intento 1)\n",
        "Se entrena un RandomForestClassifier utilizando los componentes principales. El parámetro `class_weight='balanced'` se usa para manejar el posible desbalance de clases en la variable 'Mora'."
      ],
      "metadata": {
        "id": "train_eval_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_leak = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "model_leak.fit(X_train_pca_leak, y_train_leak)\n",
        "\n",
        "y_pred_leak = model_leak.predict(X_test_pca_leak)\n",
        "\n",
        "print(\"--- Evaluación del Modelo (Intento 1 - con posible leakage) ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_leak, y_pred_leak):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_leak, y_pred_leak))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_leak, y_pred_leak))"
      ],
      "metadata": {
        "colab": {},
        "id": "cell11_code_content_original_cell9_output_notebook2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis del Intento 1:**\n",
        "Como se puede observar en la salida (que el notebook original generó con una accuracy de 1.0 y un reporte de clasificación perfecto), el modelo parece predecir la 'Mora' perfectamente. Esto es una fuerte indicación de **fuga de datos**, causada por la inclusión de la variable `Dias_PagoOportuno_PagoReal` que está directamente relacionada con cómo se calcula la 'Mora'."
      ],
      "metadata": {
        "id": "analysis_attempt1_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segunda prueba sin la variable Dias_PagoOportuno_PagoReal\n",
        "\n",
        "## 4. Modelado con PCA - Segundo Intento (Corrigiendo Fuga de Datos)\n",
        "\n",
        "En este segundo intento, eliminaremos la variable `Dias_PagoOportuno_PagoReal` para construir un modelo más robusto y realista."
      ],
      "metadata": {
        "id": "modeling2_intro_markdown_cell12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Preparación de Datos Sin Fuga de Datos"
      ],
      "metadata": {
        "id": "prep_no_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reutilizamos df_prediccion_ml que ya tiene 'Estrato' convertido y NaNs imputados,\n",
        "# pero esta vez eliminamos 'Dias_PagoOportuno_PagoReal'\n",
        "df_prediccion_ml_sin_leak = df_prediccion_ml.drop('Dias_PagoOportuno_PagoReal', axis=1)\n",
        "\n",
        "X_sin_leak = df_prediccion_ml_sin_leak.drop('Mora', axis=1)\n",
        "y_sin_leak = df_prediccion_ml_sin_leak['Mora']\n",
        "\n",
        "print(\"--- Características X_sin_leak (primeras filas) ---\")\n",
        "print(X_sin_leak.head())"
      ],
      "metadata": {
        "id": "cell13_code_content_original_cell10_output_notebook2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Escalado de Características y Aplicación de PCA (Sin Fuga)"
      ],
      "metadata": {
        "id": "scale_pca_no_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_sin_leak = StandardScaler()\n",
        "X_scaled_sin_leak = scaler_sin_leak.fit_transform(X_sin_leak)\n",
        "\n",
        "pca_sin_leak = PCA(n_components=0.95)\n",
        "X_pca_sin_leak = pca_sin_leak.fit_transform(X_scaled_sin_leak)\n",
        "\n",
        "print(f\"Número de componentes seleccionados por PCA (sin leakage): {pca_sin_leak.n_components_}\")\n",
        "print(f\"Varianza explicada acumulada (sin leakage): {pca_sin_leak.explained_variance_ratio_.sum():.4f}\")\n",
        "print(\"\\n--- X_pca_sin_leak (primeras 5 filas de componentes principales) ---\")\n",
        "print(pd.DataFrame(X_pca_sin_leak).head())"
      ],
      "metadata": {
        "id": "cell14_15_code_content"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 División en Conjuntos de Entrenamiento y Prueba (Sin Fuga)"
      ],
      "metadata": {
        "id": "split_no_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pca_sin_leak, X_test_pca_sin_leak, y_train_sin_leak, y_test_sin_leak = train_test_split(\n",
        "    X_pca_sin_leak, y_sin_leak, test_size=0.2, random_state=42, stratify=y_sin_leak\n",
        ")\n",
        "\n",
        "print(f\"Dimensiones de X_train_pca_sin_leak: {X_train_pca_sin_leak.shape}\")\n",
        "print(f\"Dimensiones de X_test_pca_sin_leak: {X_test_pca_sin_leak.shape}\")"
      ],
      "metadata": {
        "id": "cell16_code_content"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Entrenamiento y Evaluación del Modelo RandomForest (Intento 2 - Sin Fuga)"
      ],
      "metadata": {
        "id": "train_eval_no_leak_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sin_leak = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "model_sin_leak.fit(X_train_pca_sin_leak, y_train_sin_leak)\n",
        "\n",
        "y_pred_sin_leak = model_sin_leak.predict(X_test_pca_sin_leak)\n",
        "\n",
        "print(\"--- Evaluación del Modelo (Intento 2 - sin leakage) ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_sin_leak, y_pred_sin_leak):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_sin_leak, y_pred_sin_leak))\n",
        "\n",
        "cm_sin_leak = confusion_matrix(y_test_sin_leak, y_pred_sin_leak)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm_sin_leak)\n",
        "\n",
        "# Visualización de la Matriz de Confusión\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_sin_leak, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Mora (0)', 'Mora (1)'], yticklabels=['No Mora (0)', 'Mora (1)'])\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusión (Modelo sin Fuga de Datos y con PCA)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cell17_code_content_original_cell11_output_notebook2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis del Intento 2:**\n",
        "Al eliminar la variable que causaba la fuga de datos, el modelo ahora produce métricas de rendimiento más realistas. El accuracy y otras métricas (precision, recall, F1-score) reflejan mejor la capacidad del modelo para generalizar a datos no vistos."
      ],
      "metadata": {
        "id": "analysis_attempt2_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Estimación de la Importancia de las Características Originales Después de PCA\n",
        "\n",
        "Dado que el modelo RandomForest se entrenó con los componentes principales, no obtenemos directamente la importancia de las características originales. Sin embargo, podemos estimarla examinando cómo cada característica original contribuye a los componentes principales y cómo estos componentes, a su vez, son importantes para el modelo."
      ],
      "metadata": {
        "id": "feature_importance_pca_intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importancia de los componentes principales según el modelo RandomForest\n",
        "pca_component_importances = model_sin_leak.feature_importances_\n",
        "\n",
        "# Loadings: indican la contribución de cada característica original a cada componente principal.\n",
        "# pca_sin_leak.components_ tiene forma (n_components, n_original_features)\n",
        "original_feature_loadings_abs = np.abs(pca_sin_leak.components_)\n",
        "\n",
        "# Ponderar la contribución de cada característica a un componente por la importancia de ese componente\n",
        "weighted_loadings = original_feature_loadings_abs * pca_component_importances[:, np.newaxis]\n",
        "\n",
        "# Sumar las contribuciones ponderadas a través de todos los componentes para cada característica original\n",
        "estimated_original_feature_importances = weighted_loadings.sum(axis=0)\n",
        "\n",
        "# Normalizar para que la suma total sea 1 (opcional, para facilitar la comparación)\n",
        "estimated_original_feature_importances_normalized = estimated_original_feature_importances / estimated_original_feature_importances.sum()\n",
        "\n",
        "feature_names_sin_leak = X_sin_leak.columns\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names_sin_leak,\n",
        "    'Estimated_Importance': estimated_original_feature_importances_normalized\n",
        "}).sort_values(by='Estimated_Importance', ascending=False)\n",
        "\n",
        "print(\"--- Importancia Estimada de Características Originales (Post-PCA) ---\")\n",
        "print(importance_df)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x='Estimated_Importance', y='Feature', data=importance_df, palette='viridis')\n",
        "plt.title('Importancia Estimada de Características Originales (Post-PCA)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cell18_code_content"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretación de la Importancia Estimada:**\n",
        "Este análisis nos proporciona una visión de qué características originales tienen mayor influencia en la predicción del modelo, incluso después de la transformación PCA. Las características con una \"Importancia Estimada\" más alta son aquellas que contribuyen significativamente a los componentes principales que el modelo RandomForest consideró importantes."
      ],
      "metadata": {
        "id": "interpret_feature_importance_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusión del Tutorial\n",
        "\n",
        "En este tutorial, hemos cubierto un ciclo de modelado predictivo que incluyó:\n",
        "- Preparación de datos y codificación de características.\n",
        "- Un primer intento de modelado que nos permitió identificar y entender el concepto de fuga de datos.\n",
        "- La corrección de la fuga de datos para un segundo intento de modelado más realista.\n",
        "- La aplicación de PCA para reducción de dimensionalidad, explicando cómo se seleccionan los componentes (basado en la varianza explicada).\n",
        "- El entrenamiento y evaluación de un RandomForestClassifier sobre los datos transformados por PCA.\n",
        "- Un método para estimar la importancia de las características originales después de aplicar PCA.\n",
        "\n",
        "**Aprendizajes Clave:**\n",
        "- La **vigilancia contra la fuga de datos** es crucial. Resultados \"demasiado buenos para ser verdad\" a menudo indican este problema.\n",
        "- **PCA puede ser una herramienta útil** para reducir la dimensionalidad, lo que puede ayudar a simplificar modelos, reducir el tiempo de entrenamiento y, en algunos casos, mejorar el rendimiento al eliminar ruido. Sin embargo, su efectividad debe evaluarse caso por caso.\n",
        "- La **interpretabilidad del modelo** se vuelve más compleja con PCA, pero existen técnicas para estimar la influencia de las características originales.\n",
        "\n",
        "**Próximos Pasos Posibles:**\n",
        "- **Optimización de Hiperparámetros:** Ajustar los hiperparámetros tanto del RandomForestClassifier como de PCA (ej. el número de componentes o el umbral de varianza).\n",
        "- **Comparación:** Entrenar el mismo modelo sin PCA (solo con datos escalados) y comparar el rendimiento y el tiempo de entrenamiento para evaluar el impacto real de PCA en este problema.\n",
        "- **Explorar otros Modelos:** Probar diferentes algoritmos de clasificación."
      ],
      "metadata": {
        "id": "conclusion_final_pred_pca_markdown"
      }
    }
  ]
}