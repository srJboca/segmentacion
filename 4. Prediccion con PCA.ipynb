{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Predicción de Mora con PCA y RandomForest\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Este notebook te guiará a través de la construcción de un modelo para predecir la 'Mora' (incumplimiento de pago) de los clientes. Exploraremos el uso del Análisis de Componentes Principales (PCA) para la reducción de dimensionalidad antes de entrenar un clasificador RandomForest.\n",
        "\n",
        "**Puntos Clave del Tutorial:**\n",
        "1.  Preparación de datos para la predicción.\n",
        "2.  Un primer intento de modelado, donde identificaremos un problema común: la fuga de datos (data leakage).\n",
        "3.  Corrección del problema de fuga de datos y un segundo intento de modelado.\n",
        "4.  Aplicación de PCA para reducir la dimensionalidad del conjunto de características.\n",
        "5.  Entrenamiento y evaluación de un RandomForestClassifier utilizando los componentes principales.\n",
        "6.  Un método para estimar la importancia de las características originales después de aplicar PCA.\n",
        "\n",
        "Este enfoque iterativo es común en el desarrollo de modelos de machine learning."
      ],
      "metadata": {
        "id": "intro_pred_pca_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuración del Entorno y Carga de Datos"
      ],
      "metadata": {
        "id": "setup_pred_pca_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Importación de Librerías"
      ],
      "metadata": {
        "id": "import_libs_pred_pca_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldvwepAOlLTC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Descarga y Carga del DataFrame `df_analisis`\n",
        "\n",
        "Usaremos `df_analisis.parquet`, que contiene datos detallados de facturas y pagos, previamente procesado."
      ],
      "metadata": {
        "id": "load_data_pred_pca_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuyjTblClMGH"
      },
      "outputs": [],
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/df_analisis.parquet\n",
        "df_analisis = pd.read_parquet('df_analisis.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preparación Inicial de Datos para Predicción\n",
        "\n",
        "Seleccionaremos las características relevantes y realizaremos las transformaciones necesarias."
      ],
      "metadata": {
        "id": "initial_prep_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initial_prep_code_df_prediccion"
      },
      "outputs": [],
      "source": [
        "# Eliminar la columna 'mora' (minúsculas) si existe 'Mora' (mayúsculas) para evitar confusión\n",
        "if 'mora' in df_analisis.columns and 'Mora' in df_analisis.columns:\n",
        "    df_analisis = df_analisis.drop(columns=['mora'])\n",
        "elif 'mora' in df_analisis.columns and 'Mora' not in df_analisis.columns:\n",
        "    df_analisis.rename(columns={'mora': 'Mora'}, inplace=True)\n",
        "\n",
        "# Selección inicial de características para el DataFrame de predicción\n",
        "df_prediccion = df_analisis[[\n",
        "    'Numero de factura', # Se eliminará más adelante, útil para inspección inicial\n",
        "    'Consumo (m3)', \n",
        "    'Estrato', \n",
        "    'Precio por Consumo', \n",
        "    'Dias_Emision_PagoOportuno', \n",
        "    'Dias_Lectura_Emision', \n",
        "    'Dias_PagoOportuno_PagoReal', # Potencial fuente de data leakage\n",
        "    'Mora' # Variable objetivo\n",
        "]].copy()\n",
        "\n",
        "print(\"--- df_prediccion (primeras filas) ---\")\n",
        "print(df_prediccion.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Preprocesamiento Básico\n",
        "* Eliminar identificadores ('Numero de factura').\n",
        "* Convertir 'Estrato' a numérico.\n",
        "* Manejar valores faltantes (NaN) mediante imputación con la media."
      ],
      "metadata": {
        "id": "basic_preprocessing_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic_preprocessing_code"
      },
      "outputs": [],
      "source": [
        "df_prediccion_ml = df_prediccion.drop('Numero de factura', axis=1).copy()\n",
        "\n",
        "# Convertir 'Estrato' a numérico ordinal\n",
        "if df_prediccion_ml['Estrato'].dtype == 'object' or isinstance(df_prediccion_ml['Estrato'].dtype, pd.CategoricalDtype):\n",
        "    df_prediccion_ml['Estrato'] = df_prediccion_ml['Estrato'].str.replace('Estrato ', '', regex=False).astype(int)\n",
        "else:\n",
        "    df_prediccion_ml['Estrato'] = df_prediccion_ml['Estrato'].astype(int)\n",
        "\n",
        "# Imputar NaNs con la media de cada columna (estrategia del notebook original)\n",
        "for col in df_prediccion_ml.columns:\n",
        "    if df_prediccion_ml[col].isnull().any():\n",
        "        df_prediccion_ml[col].fillna(df_prediccion_ml[col].mean(), inplace=True)\n",
        "\n",
        "print(\"--- df_prediccion_ml después de preprocesamiento básico (primeras filas) ---\")\n",
        "print(df_prediccion_ml.head())\n",
        "print(\"\\n--- Valores faltantes restantes ---\")\n",
        "print(df_prediccion_ml.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modelado (Intento 1): Con Posible Fuga de Datos\n",
        "\n",
        "En este primer intento, usaremos todas las características preparadas, incluyendo `Dias_PagoOportuno_PagoReal`. Esta columna se deriva directamente de la fecha de pago real y la fecha de pago oportuno, y la variable 'Mora' también se define a partir de esta diferencia. Esto puede causar **fuga de datos (data leakage)**, donde el modelo aprende una relación demasiado directa y obtiene un rendimiento artificialmente alto.\n"
      ],
      "metadata": {
        "id": "modeling_attempt1_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Definición de X e y, Escalado y PCA"
      ],
      "metadata": {
        "id": "attempt1_prep_pca_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "attempt1_prep_pca_code"
      },
      "outputs": [],
      "source": [
        "X_leak = df_prediccion_ml.drop('Mora', axis=1)\n",
        "y_leak = df_prediccion_ml['Mora']\n",
        "\n",
        "# Escalado de características\n",
        "scaler_leak = StandardScaler()\n",
        "X_scaled_leak = scaler_leak.fit_transform(X_leak)\n",
        "\n",
        "# Aplicación de PCA\n",
        "# n_components=0.95 significa que PCA seleccionará el número de componentes \n",
        "# que explican el 95% de la varianza en los datos.\n",
        "pca_leak = PCA(n_components=0.95)\n",
        "X_pca_leak = pca_leak.fit_transform(X_scaled_leak)\n",
        "\n",
        "print(f\"Número de componentes seleccionados por PCA (con leakage): {pca_leak.n_components_}\")\n",
        "print(f\"Varianza explicada por los componentes: {pca_leak.explained_variance_ratio_}\")\n",
        "print(f\"Varianza explicada acumulada: {pca_leak.explained_variance_ratio_.sum():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 División de Datos, Entrenamiento y Evaluación (Intento 1)"
      ],
      "metadata": {
        "id": "attempt1_train_eval_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "attempt1_train_eval_code"
      },
      "outputs": [],
      "source": [
        "X_train_pca_leak, X_test_pca_leak, y_train_leak, y_test_leak = train_test_split(\n",
        "    X_pca_leak, y_leak, test_size=0.2, random_state=42, stratify=y_leak\n",
        ")\n",
        "\n",
        "model_leak = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "model_leak.fit(X_train_pca_leak, y_train_leak)\n",
        "\n",
        "y_pred_leak = model_leak.predict(X_test_pca_leak)\n",
        "\n",
        "print(\"--- Evaluación del Modelo (Intento 1 - con posible leakage) ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_leak, y_pred_leak):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_leak, y_pred_leak))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_leak, y_pred_leak))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observación del Intento 1:**\n",
        "Los resultados (probablemente un accuracy perfecto o cercano al 100%) sugieren fuertemente la presencia de fuga de datos. La característica `Dias_PagoOportuno_PagoReal` contiene información que define directamente la variable `Mora`."
      ],
      "metadata": {
        "id": "attempt1_observation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modelado (Intento 2): Corrigiendo la Fuga de Datos\n",
        "\n",
        "Ahora, eliminaremos la característica `Dias_PagoOportuno_PagoReal` de nuestro conjunto de datos `X` para evitar la fuga de datos y construir un modelo más realista y útil."
      ],
      "metadata": {
        "id": "modeling_attempt2_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Preparación de Datos Sin Fuga, Escalado y PCA"
      ],
      "metadata": {
        "id": "attempt2_prep_pca_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "attempt2_prep_pca_code"
      },
      "outputs": [],
      "source": [
        "df_prediccion_ml_sin_leak = df_prediccion_ml.drop('Dias_PagoOportuno_PagoReal', axis=1)\n",
        "\n",
        "X_sin_leak = df_prediccion_ml_sin_leak.drop('Mora', axis=1)\n",
        "y_sin_leak = df_prediccion_ml_sin_leak['Mora']\n",
        "\n",
        "# Escalado de características\n",
        "scaler_sin_leak = StandardScaler()\n",
        "X_scaled_sin_leak = scaler_sin_leak.fit_transform(X_sin_leak)\n",
        "\n",
        "# Aplicación de PCA\n",
        "pca_sin_leak = PCA(n_components=0.95) # Mantener 95% de la varianza\n",
        "X_pca_sin_leak = pca_sin_leak.fit_transform(X_scaled_sin_leak)\n",
        "\n",
        "print(f\"Número de componentes seleccionados por PCA (sin leakage): {pca_sin_leak.n_components_}\")\n",
        "print(f\"Varianza explicada por los componentes: {pca_sin_leak.explained_variance_ratio_}\")\n",
        "print(f\"Varianza explicada acumulada: {pca_sin_leak.explained_variance_ratio_.sum():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 División de Datos, Entrenamiento y Evaluación (Intento 2)"
      ],
      "metadata": {
        "id": "attempt2_train_eval_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "attempt2_train_eval_code"
      },
      "outputs": [],
      "source": [
        "X_train_pca_sin_leak, X_test_pca_sin_leak, y_train_sin_leak, y_test_sin_leak = train_test_split(\n",
        "    X_pca_sin_leak, y_sin_leak, test_size=0.2, random_state=42, stratify=y_sin_leak\n",
        ")\n",
        "\n",
        "model_sin_leak = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "model_sin_leak.fit(X_train_pca_sin_leak, y_train_sin_leak)\n",
        "\n",
        "y_pred_sin_leak = model_sin_leak.predict(X_test_pca_sin_leak)\n",
        "\n",
        "print(\"--- Evaluación del Modelo (Intento 2 - sin leakage) ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_sin_leak, y_pred_sin_leak):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_sin_leak, y_pred_sin_leak))\n",
        "\n",
        "cm_sin_leak = confusion_matrix(y_test_sin_leak, y_pred_sin_leak)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm_sin_leak)\n",
        "\n",
        "# Visualización de la Matriz de Confusión\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_sin_leak, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Mora (0)', 'Mora (1)'], yticklabels=['No Mora (0)', 'Mora (1)'])\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusión (Modelo sin Fuga de Datos)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observación del Intento 2:**\n",
        "Los resultados de este segundo modelo son más realistas y reflejan el verdadero poder predictivo de las características restantes después de aplicar PCA. Aunque el accuracy puede ser menor que en el intento con fuga de datos, este modelo es el que proporcionaría valor real en un escenario de producción."
      ],
      "metadata": {
        "id": "attempt2_observation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Estimación de la Importancia de las Características Originales Post-PCA\n",
        "\n",
        "El modelo RandomForest se entrenó con los componentes principales, no con las características originales. Sin embargo, podemos intentar estimar la importancia de las características originales analizando cómo contribuyen a los componentes principales más importantes.\n",
        "\n",
        "**Método:**\n",
        "1.  Obtener la importancia de cada componente principal del modelo RandomForest.\n",
        "2.  Obtener los \"loadings\" de PCA (`pca.components_`), que indican cuánto contribuye cada característica original a cada componente principal (en valor absoluto).\n",
        "3.  Multiplicar la importancia de cada componente por el valor absoluto de sus loadings y sumar estas contribuciones para cada característica original."
      ],
      "metadata": {
        "id": "feature_importance_pca_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_importance_pca_code"
      },
      "outputs": [],
      "source": [
        "# Importancia de los componentes principales del modelo\n",
        "pca_component_importances = model_sin_leak.feature_importances_\n",
        "\n",
        "# Loadings (contribución de características originales a cada componente)\n",
        "# pca_sin_leak.components_ tiene forma (n_components, n_features_originales)\n",
        "original_feature_loadings = np.abs(pca_sin_leak.components_)\n",
        "\n",
        "# Ponderar los loadings por la importancia del componente\n",
        "weighted_loadings = original_feature_loadings * pca_component_importances[:, np.newaxis]\n",
        "\n",
        "# Sumar las contribuciones ponderadas para cada característica original\n",
        "estimated_original_feature_importances = weighted_loadings.sum(axis=0)\n",
        "\n",
        "# Normalizar para que sumen 1 (opcional, para comparación)\n",
        "estimated_original_feature_importances_normalized = estimated_original_feature_importances / estimated_original_feature_importances.sum()\n",
        "\n",
        "feature_names_sin_leak = X_sin_leak.columns\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names_sin_leak,\n",
        "    'Estimated_Importance': estimated_original_feature_importances_normalized\n",
        "}).sort_values(by='Estimated_Importance', ascending=False)\n",
        "\n",
        "print(\"--- Importancia Estimada de Características Originales Post-PCA ---\")\n",
        "print(importance_df)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.barplot(x='Estimated_Importance', y='Feature', data=importance_df)\n",
        "plt.title('Importancia Estimada de Características Originales (Post-PCA)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretación de la Importancia Estimada:**\n",
        "Este gráfico y tabla nos dan una idea de qué características originales fueron las más influyentes en el modelo final, incluso después de la transformación PCA. Las características con mayor importancia estimada son aquellas que tienen una fuerte presencia en los componentes principales que, a su vez, fueron importantes para el clasificador."
      ],
      "metadata": {
        "id": "interpret_feat_imp_pca_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusiones y Próximos Pasos\n",
        "\n",
        "En este tutorial, hemos:\n",
        "1.  Preparado datos para un modelo de predicción de 'Mora'.\n",
        "2.  Demostrado la importancia de identificar y corregir la fuga de datos (`Dias_PagoOportuno_PagoReal`).\n",
        "3.  Aplicado PCA para reducir la dimensionalidad, seleccionando componentes que explican el 95% de la varianza.\n",
        "4.  Entrenado un RandomForestClassifier con los componentes principales.\n",
        "5.  Evaluado el modelo y obtenido métricas de rendimiento más realistas.\n",
        "6.  Estimado la importancia de las características originales después de PCA.\n",
        "\n",
        "**Resultados Clave:**\n",
        "* El modelo inicial con fuga de datos mostró un rendimiento perfecto, lo cual es una señal de alarma.\n",
        "* El modelo corregido, sin la característica que causaba la fuga y utilizando PCA, arrojó un rendimiento (especificar accuracy y F1-score de la clase 'Mora'=1 según los resultados que se obtendrían) que refleja mejor la capacidad predictiva real.\n",
        "* PCA redujo el número de características de (número original de `X_sin_leak.shape[1]`) a (número de `pca_sin_leak.n_components_`), simplificando potencialmente el modelo.\n",
        "* El análisis de importancia de características post-PCA nos da una idea de qué factores originales son más relevantes.\n",
        "\n",
        "**Próximos Pasos Sugeridos:**\n",
        "* **Optimización de Hiperparámetros:** Tanto para RandomForest como para PCA (e.g., `n_components` podría ajustarse usando cross-validation o explorando diferentes umbrales de varianza explicada).\n",
        "* **Comparación de Modelos:** Probar otros algoritmos de clasificación con y sin PCA.\n",
        "* **Ingeniería de Características Adicional:** Crear nuevas características que podrían mejorar la predicción de la mora.\n",
        "* **Análisis de Impacto de PCA:** Entrenar un modelo con el conjunto de características escaladas *sin* PCA y comparar su rendimiento (y tiempo de entrenamiento) con el modelo que usa PCA. Esto ayudaría a cuantificar el beneficio o costo de usar PCA en este problema específico.\n",
        "* **Interpretabilidad del Modelo:** Si la interpretabilidad es crucial, explorar modelos más simples o técnicas de explicación de modelos (como SHAP) aplicadas a los componentes o, con más cuidado, a las características originales reconstruidas."
      ],
      "metadata": {
        "id": "conclusion_pred_pca_markdown"
      }
    }
  ]
}