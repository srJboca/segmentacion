{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srJboca/segmentacion/blob/main/EN/1.%20Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Exploring Customer and Gas Billing Data\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to this tutorial on data exploration. We will work with a dataset that simulates information from a gas distribution company. The objective is to clean, combine, and explore this data to better understand customers, their consumption, and payment behaviors. This process is fundamental before performing more advanced tasks such as customer segmentation, predictive consumption modeling, or delinquency analysis.\n",
        "\n",
        "The data is divided into four files:\n",
        "1.  `clientes.parquet`: Information about customers (contracts, demographic data).\n",
        "2.  `facturas.parquet`: Details of issued invoices (consumption, dates).\n",
        "3.  `precios_gas.parquet`: Gas prices per mÂ³ according to stratum, year, and month.\n",
        "4.  `recaudo.parquet`: Information about payments made for the invoices.\n",
        "\n",
        "## Phase 1: Environment Setup and Data Loading\n",
        "\n",
        "### 1.1 Importing Libraries\n",
        "\n",
        "First, we will import the necessary libraries.\n",
        "* `pandas` for data manipulation and analysis.\n",
        "* `matplotlib.pyplot` and `seaborn` for data visualization.\n",
        "* `warnings` to manage any warnings that may arise."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs_code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Settings for visualizations\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Downloading the Data Files\n",
        "\n",
        "We will download the Parquet files from the GitHub repository."
      ],
      "metadata": {
        "id": "download_data_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data_code"
      },
      "outputs": [],
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/clientes.parquet\n",
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/facturas.parquet\n",
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/precios_gas.parquet\n",
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/recaudo.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Loading Data into Pandas DataFrames\n",
        "\n",
        "Now, we will load each Parquet file into a Pandas DataFrame."
      ],
      "metadata": {
        "id": "load_data_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data_code"
      },
      "outputs": [],
      "source": [
        "df_clients = pd.read_parquet('clientes.parquet')\n",
        "df_invoices = pd.read_parquet('facturas.parquet')\n",
        "df_collections = pd.read_parquet('recaudo.parquet')\n",
        "df_gas_prices = pd.read_parquet('precios_gas.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2: Initial Data Inspection\n",
        "\n",
        "We will perform a basic inspection of each DataFrame to understand its structure, data types, and check for null values or initial descriptive statistics."
      ],
      "metadata": {
        "id": "initial_inspection_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Clients DataFrame (`df_clients`)"
      ],
      "metadata": {
        "id": "inspect_clientes_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_clientes_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_clients Information ---\")\n",
        "df_clients.info()\n",
        "print(\"\\n--- First 5 rows of df_clients ---\")\n",
        "print(df_clients.head())\n",
        "print(\"\\n--- Null values in df_clients ---\")\n",
        "print(df_clients.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_clients ---\")\n",
        "print(df_clients.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_clients ---\")\n",
        "for col in df_clients.columns:\n",
        "    print(f\"Column '{col}': {df_clients[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_clients`:**\n",
        "* Contains personal and contractual information of the clients.\n",
        "* Columns like `Ciudad` and `Estrato socioeconomico` are categorical and could be important for segmentation.\n",
        "* The date columns are object types; we will need to convert them to `datetime`.\n",
        "* `Numero de contrato` appears to be the unique identifier."
      ],
      "metadata": {
        "id": "obs_clientes_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Invoices DataFrame (`df_invoices`)"
      ],
      "metadata": {
        "id": "inspect_facturas_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_facturas_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_invoices Information ---\")\n",
        "df_invoices.info()\n",
        "print(\"\\n--- First 5 rows of df_invoices ---\")\n",
        "print(df_invoices.head())\n",
        "print(\"\\n--- Null values in df_invoices ---\")\n",
        "print(df_invoices.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_invoices ---\")\n",
        "print(df_invoices.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_invoices ---\")\n",
        "for col in df_invoices.columns:\n",
        "    print(f\"Column '{col}': {df_invoices[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_invoices`:**\n",
        "* Contains details of each invoice, including consumption and relevant dates.\n",
        "* `Numero de factura` is the identifier for the invoice, and `Numero de contrato` links it to the client.\n",
        "* `Consumo (m3)` is a key numerical variable.\n",
        "* The date columns are also object types."
      ],
      "metadata": {
        "id": "obs_facturas_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Collections DataFrame (`df_collections`)"
      ],
      "metadata": {
        "id": "inspect_recaudo_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_recaudo_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_collections Information ---\")\n",
        "df_collections.info()\n",
        "print(\"\\n--- First 5 rows of df_collections ---\")\n",
        "print(df_collections.head())\n",
        "print(\"\\n--- Null values in df_collections ---\")\n",
        "print(df_collections.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_collections ---\")\n",
        "print(df_collections.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_collections ---\")\n",
        "for col in df_collections.columns:\n",
        "    print(f\"Column '{col}': {df_collections[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_collections`:**\n",
        "* Records the date on which an invoice was paid.\n",
        "* Joins with `df_invoices` using `Numero de factura`.\n",
        "* `Fecha de Pago Real` will need conversion to `datetime`."
      ],
      "metadata": {
        "id": "obs_recaudo_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Gas Prices DataFrame (`df_gas_prices`)"
      ],
      "metadata": {
        "id": "inspect_precios_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_precios_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_gas_prices Information ---\")\n",
        "df_gas_prices.info()\n",
        "print(\"\\n--- First 5 rows of df_gas_prices ---\")\n",
        "print(df_gas_prices.head())\n",
        "print(\"\\n--- Null values in df_gas_prices ---\")\n",
        "print(df_gas_prices.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_gas_prices ---\")\n",
        "print(df_gas_prices.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_gas_prices ---\")\n",
        "for col in df_gas_prices.columns:\n",
        "    print(f\"Column '{col}': {df_gas_prices[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_gas_prices`:**\n",
        "* Defines the price per mÂ³ of gas according to `AÃ±o`, `Mes`, and `Estrato`.\n",
        "* `Precio m3 (COP)` is the numerical price value.\n",
        "* `Estrato` is categorical."
      ],
      "metadata": {
        "id": "obs_precios_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3: Merging the Data\n",
        "\n",
        "To get a consolidated view, we will combine these DataFrames."
      ],
      "metadata": {
        "id": "merge_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Joining Invoices with Customer Information\n",
        "\n",
        "We join `df_invoices` with `df_clients` using `Numero de contrato` as the key."
      ],
      "metadata": {
        "id": "merge_factura_cliente_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_factura_cliente_code"
      },
      "outputs": [],
      "source": [
        "df_invoice_client = pd.merge(df_invoices, df_clients, on='Numero de contrato', how='left')\n",
        "print(\"--- df_invoice_client Information (Invoices + Clients) ---\")\n",
        "df_invoice_client.info()\n",
        "print(\"\\n--- First 5 rows of df_invoice_client ---\")\n",
        "print(df_invoice_client.head())\n",
        "print(f\"\\nShape of df_invoices: {df_invoices.shape}\")\n",
        "print(f\"Shape of df_invoice_client: {df_invoice_client.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verification:** The number of rows should be equal to that of `df_invoices` if each invoice has a corresponding customer (using `how='left'`). If it increases, it could indicate duplicates in `df_clients` by `Numero de contrato`, which should not happen."
      ],
      "metadata": {
        "id": "verify_merge1_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Adding Prices to Invoices\n",
        "\n",
        "Now we join `df_invoice_client` with `df_gas_prices`. The join is done using `AÃ±o`, `Mes`, and the customer's socioeconomic stratum."
      ],
      "metadata": {
        "id": "merge_precios_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_precios_code"
      },
      "outputs": [],
      "source": [
        "df_invoice_client_price = pd.merge(df_invoice_client,\n",
        "                                     df_gas_prices,\n",
        "                                     left_on=['AÃ±o', 'Mes', 'Estrato socioeconomico'],\n",
        "                                     right_on=['AÃ±o', 'Mes', 'Estrato'],\n",
        "                                     how='left')\n",
        "\n",
        "print(\"--- df_invoice_client_price Information (Invoices + Clients + Prices) ---\")\n",
        "df_invoice_client_price.info()\n",
        "print(\"\\n--- First 5 rows of df_invoice_client_price ---\")\n",
        "print(df_invoice_client_price.head())\n",
        "print(f\"\\nShape of df_invoice_client: {df_invoice_client.shape}\")\n",
        "print(f\"Shape of df_invoice_client_price: {df_invoice_client_price.shape}\")\n",
        "\n",
        "# Check for rows where the price could not be assigned (NaN in 'Precio m3 (COP)')\n",
        "print(\"\\n--- Rows without an assigned price ---\")\n",
        "print(df_invoice_client_price[df_invoice_client_price['Precio m3 (COP)'].isnull()].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verification and Potential Issues:**\n",
        "* Again, the number of rows should be consistent. If it increases drastically, it might indicate that the join keys `['AÃ±o', 'Mes', 'Estrato socioeconomico']` are not unique in `df_gas_prices` for a given combination, or that there are multiple invoices for the same client in the same month with the same stratum being mapped (which is expected).\n",
        "* The `Estrato` column from `df_gas_prices` is redundant after the merge and can be dropped.\n",
        "* It's crucial to check if there are any invoices to which a price could not be assigned. This could happen if a combination of `AÃ±o, Mes, Estrato socioeconomico` from the invoices does not exist in the price table."
      ],
      "metadata": {
        "id": "verify_merge2_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Incorporating Payment Information (Collections)\n",
        "\n",
        "Finally, we join the payment information from `df_collections`."
      ],
      "metadata": {
        "id": "merge_recaudo_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_recaudo_code"
      },
      "outputs": [],
      "source": [
        "df_complete = pd.merge(df_invoice_client_price,\n",
        "                       df_collections,\n",
        "                       on='Numero de factura',\n",
        "                       how='left') # We use a left join to keep all invoices, even if they don't have a recorded payment\n",
        "\n",
        "print(\"--- df_complete Information (All data joined) ---\")\n",
        "df_complete.info()\n",
        "print(\"\\n--- First 5 rows of df_complete ---\")\n",
        "print(df_complete.head())\n",
        "print(f\"\\nShape of df_invoice_client_price: {df_invoice_client_price.shape}\")\n",
        "print(f\"Shape of df_complete: {df_complete.shape}\")\n",
        "\n",
        "# Check for invoices without an actual payment date\n",
        "print(\"\\n--- Invoices without actual payment date (NaN in 'Fecha de Pago Real') ---\")\n",
        "print(df_complete[df_complete['Fecha de Pago Real'].isnull()][['Numero de factura', 'Fecha de Emision', 'Fecha de Pago Oportuno']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verification:**\n",
        "* The number of rows should remain the same or increase if an invoice has multiple payment records (uncommon, unless they are non-consolidated partial payments). A `left join` ensures all invoices are kept.\n",
        "* Invoices without a corresponding `Fecha de Pago Real` will have `NaN` in that column. This is expected for unpaid invoices or those whose payment has not yet been recorded."
      ],
      "metadata": {
        "id": "verify_merge3_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Column Selection and Handling Duplicates\n",
        "\n",
        "The final analysis DataFrame will use a subset of columns. We will select these columns and then handle any potential duplicates that may have been generated during the merges, especially if the keys were not perfectly unique or if the same information was joined multiple times."
      ],
      "metadata": {
        "id": "select_cols_dups_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "select_cols_dups_code"
      },
      "outputs": [],
      "source": [
        "selected_columns = [\n",
        "    'Numero de factura', 'Numero de contrato', 'Fecha de Emision', 'Consumo (m3)',\n",
        "    'Fecha de Pago Oportuno', 'Fecha de Lectura', 'Fecha de Suspension Estimada',\n",
        "    'Ciudad', 'Estrato socioeconomico',\n",
        "    'Precio m3 (COP)', 'Fecha de Pago Real'\n",
        "]\n",
        "\n",
        "if 'Estrato' in df_complete.columns and 'Estrato socioeconomico' in df_complete.columns:\n",
        "    df_complete = df_complete.drop(columns=['Estrato'])\n",
        "\n",
        "# Create the analysis DataFrame with selected columns\n",
        "df_analysis = df_complete[selected_columns].copy()\n",
        "\n",
        "print(\"--- First rows of df_analysis (before handling duplicates) ---\")\n",
        "print(df_analysis.head())\n",
        "print(f\"\\nShape of df_analysis before drop_duplicates: {df_analysis.shape}\")\n",
        "\n",
        "num_duplicates_before = df_analysis.duplicated().sum()\n",
        "print(f\"Number of exact duplicate rows before: {num_duplicates_before}\")\n",
        "\n",
        "df_analysis = df_analysis.drop_duplicates()\n",
        "print(f\"\\nShape of df_analysis after drop_duplicates: {df_analysis.shape}\")\n",
        "num_duplicates_after = df_analysis.duplicated().sum()\n",
        "print(f\"Number of exact duplicate rows after: {num_duplicates_after}\")\n",
        "\n",
        "print(\"\\n--- First rows of df_analysis (after handling duplicates) ---\")\n",
        "print(df_analysis.head())\n",
        "df_analysis.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note on Duplicates:**\n",
        "The `drop_duplicates()` step is crucial to ensure the integrity of the analysis. It's important to investigate *why* duplicates were generated (e.g., non-unique merge keys? source data with duplicates?). For now, we have removed them."
      ],
      "metadata": {
        "id": "note_duplicates_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 4: Feature Engineering\n",
        "\n",
        "We will create new columns from existing ones to enrich our analysis."
      ],
      "metadata": {
        "id": "feature_eng_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Date Conversion\n",
        "\n",
        "We convert the date columns to `datetime` format."
      ],
      "metadata": {
        "id": "convert_dates_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_dates_code"
      },
      "outputs": [],
      "source": [
        "date_cols = ['Fecha de Emision', 'Fecha de Pago Oportuno', 'Fecha de Lectura', 'Fecha de Pago Real', 'Fecha de Suspension Estimada']\n",
        "for col in date_cols:\n",
        "    df_analysis[col] = pd.to_datetime(df_analysis[col], errors='coerce')\n",
        "\n",
        "print(\"--- Data types after converting dates ---\")\n",
        "df_analysis.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Calculating the Invoice Amount (Price per Consumption)"
      ],
      "metadata": {
        "id": "calc_precio_consumo_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calc_precio_consumo_code"
      },
      "outputs": [],
      "source": [
        "df_analysis['Price_per_Consumption'] = df_analysis['Precio m3 (COP)'] * df_analysis['Consumo (m3)']\n",
        "print(\"\\n--- df_analysis with 'Price_per_Consumption' ---\")\n",
        "print(df_analysis[['Consumo (m3)', 'Precio m3 (COP)', 'Price_per_Consumption']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Calculating Time Differences\n",
        "\n",
        "We will calculate the number of days between different key events."
      ],
      "metadata": {
        "id": "calc_dias_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calc_dias_code"
      },
      "outputs": [],
      "source": [
        "df_analysis['Days_Issue_DueDate'] = (df_analysis['Fecha de Pago Oportuno'] - df_analysis['Fecha de Emision']).dt.days\n",
        "df_analysis['Days_Reading_Issue'] = (df_analysis['Fecha de Emision'] - df_analysis['Fecha de Lectura']).dt.days\n",
        "df_analysis['Days_DueDate_ActualPayment'] = (df_analysis['Fecha de Pago Real'] - df_analysis['Fecha de Pago Oportuno']).dt.days\n",
        "\n",
        "print(\"\\n--- df_analysis with new days features ---\")\n",
        "print(df_analysis[['Fecha de Emision', 'Fecha de Pago Oportuno', 'Fecha de Lectura', 'Fecha de Pago Real',\n",
        "                   'Days_Issue_DueDate', 'Days_Reading_Issue', 'Days_DueDate_ActualPayment']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Identifying Late Payments\n",
        "\n",
        "We create a binary column `Late_Payment`: 1 if the payment was made after the due date, 0 if it was paid on time or before. It is considered 0 if there is no actual payment date (NaT)."
      ],
      "metadata": {
        "id": "calc_mora_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calc_mora_code"
      },
      "outputs": [],
      "source": [
        "df_analysis['Late_Payment'] = 0\n",
        "df_analysis.loc[df_analysis['Days_DueDate_ActualPayment'] > 0, 'Late_Payment'] = 1\n",
        "df_analysis.loc[df_analysis['Days_DueDate_ActualPayment'].isnull(), 'Late_Payment'] = 0 # Assume not late if not paid yet\n",
        "\n",
        "print(\"\\n--- df_analysis with the 'Late_Payment' column ---\")\n",
        "print(df_analysis[['Fecha de Pago Oportuno', 'Fecha de Pago Real', 'Days_DueDate_ActualPayment', 'Late_Payment']].head(10))\n",
        "print(\"\\nValue counts in 'Late_Payment':\")\n",
        "print(df_analysis['Late_Payment'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 5: Detailed Exploration of the Consolidated DataFrame (`df_analysis`)\n",
        "\n",
        "Now that we have a clean and enriched DataFrame, we can explore it in more depth."
      ],
      "metadata": {
        "id": "eda_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 General Summary and Null Values"
      ],
      "metadata": {
        "id": "eda_summary_nulls_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eda_summary_nulls_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- General information of df_analysis ---\")\n",
        "df_analysis.info()\n",
        "\n",
        "print(\"\\n--- Null values in df_analysis ---\")\n",
        "print(df_analysis.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Descriptive statistics of df_analysis ---\")\n",
        "print(df_analysis.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on null values:**\n",
        "* `Fecha de Suspension Estimada`: May have nulls if not all invoices have this date.\n",
        "* `Precio m3 (COP)` and `Price_per_Consumption`: Nulls here would indicate problems in the merge with `df_gas_prices` or missing data in the price table.\n",
        "* `Fecha de Pago Real` and `Days_DueDate_ActualPayment`: Nulls are expected for unpaid invoices.\n",
        "\n",
        "It's important to decide how to handle these nulls. For some variables, they could be imputed; for others (like `Fecha de Pago Real`), their absence is informative."
      ],
      "metadata": {
        "id": "obs_nulls_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Distribution of Key Numerical Variables"
      ],
      "metadata": {
        "id": "dist_numeric_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dist_numeric_code"
      },
      "outputs": [],
      "source": [
        "numerical_cols_to_plot = ['Consumo (m3)', 'Precio m3 (COP)', 'Price_per_Consumption',\n",
        "                           'Days_Issue_DueDate', 'Days_Reading_Issue', 'Days_DueDate_ActualPayment']\n",
        "\n",
        "for col in numerical_cols_to_plot:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df_analysis[col].dropna(), kde=True, bins=30) # dropna() to avoid errors with NaT/NaN in histplot\n",
        "    plt.title(f'Distribution of {col}')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(y=df_analysis[col].dropna())\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Statistics for {col}:\\n{df_analysis[col].describe()}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of distributions:**\n",
        "* **Consumo (m3):** Observe the shape of the distribution. Is it symmetrical, skewed? Are there outliers (very high or low consumption)?\n",
        "* **Precio m3 (COP):** Does it vary much? This will depend on the socioeconomic strata and temporal evolution.\n",
        "* **Price_per_Consumption:** Similar to consumption, but scaled by price.\n",
        "* **Days Variables:**\n",
        "    * `Days_Issue_DueDate`: The time allowed for payment. Is it constant?\n",
        "    * `Days_Reading_Issue`: Time between meter reading and invoice issuance. Is it consistent?\n",
        "    * `Days_DueDate_ActualPayment`: Positive values indicate late payment. Negative values, early payment. Zero, payment on the due date. The boxplot can show the magnitude of the delay or advance."
      ],
      "metadata": {
        "id": "interpret_dist_numeric_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Distribution of Key Categorical Variables"
      ],
      "metadata": {
        "id": "dist_categorical_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dist_categorical_code"
      },
      "outputs": [],
      "source": [
        "categorical_cols_to_plot = ['Ciudad', 'Estrato socioeconomico', 'Late_Payment']\n",
        "\n",
        "for col in categorical_cols_to_plot:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(data=df_analysis, x=col, order=df_analysis[col].value_counts(dropna=False).index)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "    print(f\"Counts for {col}:\\n{df_analysis[col].value_counts(dropna=False)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of categorical distributions:**\n",
        "* **Ciudad:** How are the invoices/customers distributed by city?\n",
        "* **Estrato socioeconomico:** Which strata are the most common?\n",
        "* **Late_Payment:** What proportion of invoices are paid late?"
      ],
      "metadata": {
        "id": "interpret_dist_categorical_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Correlation Analysis (Numerical Variables)"
      ],
      "metadata": {
        "id": "correlation_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation_code"
      },
      "outputs": [],
      "source": [
        "numerical_df = df_analysis.select_dtypes(include=['number'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_matrix = numerical_df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix of Numerical Variables')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Pairs with highest (absolute) correlation ---\")\n",
        "corr_pairs = correlation_matrix.unstack()\n",
        "sorted_pairs = corr_pairs.sort_values(kind=\"quicksort\", ascending=False)\n",
        "# Filter for relevance (> 0.5) and remove self-correlation and duplicates\n",
        "unique_corr_pairs = sorted_pairs[(abs(sorted_pairs) < 1) & (abs(sorted_pairs) > 0.5)].drop_duplicates()\n",
        "print(unique_corr_pairs.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of the Correlation Matrix:**\n",
        "* Look for correlation coefficients close to 1 (strong positive correlation) or -1 (strong negative correlation).\n",
        "* For example, a high correlation is expected between `Consumo (m3)` and `Price_per_Consumption`.\n",
        "* Unexpected correlations can reveal interesting insights."
      ],
      "metadata": {
        "id": "interpret_correlation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 Relationships between Variables\n",
        "\n",
        "Let's explore some specific relationships."
      ],
      "metadata": {
        "id": "relationships_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.5.1 Average Consumption by Stratum and City"
      ],
      "metadata": {
        "id": "consumo_estrato_ciudad_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "consumo_estrato_ciudad_code"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_analysis, x='Estrato socioeconomico', y='Consumo (m3)', hue='Ciudad', estimator=pd.Series.mean, errorbar=None, order=sorted(df_analysis['Estrato socioeconomico'].dropna().unique()))\n",
        "plt.title('Average Consumption (m3) by Socioeconomic Stratum and City')\n",
        "plt.ylabel('Average Consumption (m3)')\n",
        "plt.xlabel('Socioeconomic Stratum')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='City')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.boxplot(data=df_analysis, x='Estrato socioeconomico', y='Consumo (m3)', hue='Ciudad', order=sorted(df_analysis['Estrato socioeconomico'].dropna().unique()))\n",
        "plt.title('Distribution of Consumption (m3) by Socioeconomic Stratum and City')\n",
        "plt.ylabel('Consumption (m3)')\n",
        "plt.xlabel('Socioeconomic Stratum')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='City')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.5.2 Late Payment Rate by Stratum and City"
      ],
      "metadata": {
        "id": "mora_estrato_ciudad_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mora_estrato_ciudad_code"
      },
      "outputs": [],
      "source": [
        "late_payment_by_stratum_city = df_analysis.groupby(['Estrato socioeconomico', 'Ciudad'])['Late_Payment'].mean().reset_index()\n",
        "late_payment_by_stratum_city = late_payment_by_stratum_city.rename(columns={'Late_Payment': 'Late_Payment_Rate'})\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=late_payment_by_stratum_city, x='Estrato socioeconomico', y='Late_Payment_Rate', hue='Ciudad', order=sorted(late_payment_by_stratum_city['Estrato socioeconomico'].dropna().unique()))\n",
        "plt.title('Late Payment Rate by Socioeconomic Stratum and City')\n",
        "plt.ylabel('Late Payment Rate (Proportion)')\n",
        "plt.xlabel('Socioeconomic Stratum')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='City')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6 Temporal Analysis (Example: Consumption over time)"
      ],
      "metadata": {
        "id": "temporal_analysis_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "temporal_analysis_code"
      },
      "outputs": [],
      "source": [
        "df_temporal = df_analysis.set_index('Fecha de Emision').sort_index()\n",
        "\n",
        "monthly_consumption = df_temporal['Consumo (m3)'].resample('ME').mean()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "monthly_consumption.plot(marker='o', linestyle='-')\n",
        "plt.title('Average Monthly Gas Consumption (m3) Over Time')\n",
        "plt.xlabel('Issue Date (Month)')\n",
        "plt.ylabel('Average Consumption (m3)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "monthly_invoices = df_temporal.resample('ME').size()\n",
        "plt.figure(figsize=(14, 7))\n",
        "monthly_invoices.plot(kind='bar')\n",
        "plt.title('Number of Invoices Issued per Month')\n",
        "plt.xlabel('Issue Date (Month)')\n",
        "plt.ylabel('Number of Invoices')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Temporal Analysis:**\n",
        "* Are there trends in consumption (increase, decrease)?\n",
        "* Is seasonality observed (peaks in certain months)?\n",
        "* The number of invoices can indicate growth in the customer base or fluctuations in billing."
      ],
      "metadata": {
        "id": "interpret_temporal_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 6: Exploration Conclusions and Next Steps\n",
        "\n",
        "In this exploratory phase, we have:\n",
        "1.  Loaded and performed an initial inspection of four data sources.\n",
        "2.  Combined the data into a unified DataFrame (`df_analysis`).\n",
        "3.  Identified and handled duplicate rows that arose during merges.\n",
        "4.  Performed feature engineering, creating new variables like `Price_per_Consumption`, time differences, and a `Late_Payment` indicator.\n",
        "5.  Explored the distributions of key numerical and categorical variables.\n",
        "6.  Analyzed correlations and relationships between variables (e.g., consumption by stratum, late payment by stratum).\n",
        "7.  Conducted a brief temporal analysis of consumption."
      ],
      "metadata": {
        "id": "conclusions_markdown"
      }
    }
  ]
}