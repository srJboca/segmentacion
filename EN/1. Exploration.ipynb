{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srJboca/segmentacion/blob/main/EN/1.%20Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Exploring Customer and Gas Billing Data\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to this tutorial on data exploration. We will work with a dataset that simulates information from a gas distribution company. The objective is to clean, combine, and explore this data to better understand customers, their consumption, and payment behaviors. This process is fundamental before performing more advanced tasks such as customer segmentation, predictive consumption modeling, or delinquency analysis.\n",
        "\n",
        "The data is divided into four files:\n",
        "1.  `clients.parquet`: Information about customers (contracts, demographic data).\n",
        "2.  `invoices.parquet`: Details of issued invoices (consumption, dates).\n",
        "3.  `gas_prices.parquet`: Gas prices per mÂ³ according to stratum, year, and month.\n",
        "4.  `collections.parquet`: Information about payments made for the invoices.\n",
        "\n",
        "## Phase 1: Environment Setup and Data Loading\n",
        "\n",
        "### 1.1 Importing Libraries\n",
        "\n",
        "First, we will import the necessary libraries.\n",
        "* `pandas` for data manipulation and analysis.\n",
        "* `matplotlib.pyplot` and `seaborn` for data visualization.\n",
        "* `warnings` to manage any warnings that may arise."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs_code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Settings for visualizations\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Downloading the Data Files\n",
        "\n",
        "We will download the Parquet files from the GitHub repository."
      ],
      "metadata": {
        "id": "download_data_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data_code"
      },
      "outputs": [],
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/clientes.parquet\n",
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/facturas.parquet\n",
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/precios_gas.parquet\n",
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/recaudo.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Loading Data into Pandas DataFrames\n",
        "\n",
        "Now, we will load each Parquet file into a Pandas DataFrame and translate the column names to English."
      ],
      "metadata": {
        "id": "load_data_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data_code"
      },
      "outputs": [],
      "source": [
        "df_clients = pd.read_parquet('clientes.parquet')\n",
        "df_invoices = pd.read_parquet('facturas.parquet')\n",
        "df_collections = pd.read_parquet('recaudo.parquet')\n",
        "df_gas_prices = pd.read_parquet('precios_gas.parquet')\n",
        "\n",
        "# Rename columns to English for consistency\n",
        "df_clients.columns = ['Contract Number', 'City', 'Socioeconomic Status', 'Contract Start Date', 'Contract Status', 'Customer Type']\n",
        "df_invoices.columns = ['Invoice Number', 'Contract Number', 'Issue Date', 'Due Date', 'Consumption (m3)', 'Reading Date', 'Estimated Suspension Date', 'Year', 'Month']\n",
        "df_collections.columns = ['Invoice Number', 'Actual Payment Date']\n",
        "df_gas_prices.columns = ['Year', 'Month', 'Status', 'Price m3 (COP)']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2: Initial Data Inspection\n",
        "\n",
        "We will perform a basic inspection of each DataFrame to understand its structure, data types, and check for null values or initial descriptive statistics."
      ],
      "metadata": {
        "id": "initial_inspection_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Clients DataFrame (`df_clients`)"
      ],
      "metadata": {
        "id": "inspect_clientes_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_clientes_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_clients Information ---\")\n",
        "df_clients.info()\n",
        "print(\"\\n--- First 5 rows of df_clients ---\")\n",
        "print(df_clients.head())\n",
        "print(\"\\n--- Null values in df_clients ---\")\n",
        "print(df_clients.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_clients ---\")\n",
        "print(df_clients.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_clients ---\")\n",
        "for col in df_clients.columns:\n",
        "    print(f\"Column '{col}': {df_clients[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_clients`:**\n",
        "* Contains personal and contractual information of the clients.\n",
        "* Columns like `City` and `Socioeconomic Status` are categorical and could be important for segmentation.\n",
        "* The date columns are object types; we will need to convert them to `datetime`.\n",
        "* `Contract Number` appears to be the unique identifier."
      ],
      "metadata": {
        "id": "obs_clientes_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Invoices DataFrame (`df_invoices`)"
      ],
      "metadata": {
        "id": "inspect_facturas_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_facturas_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_invoices Information ---\")\n",
        "df_invoices.info()\n",
        "print(\"\\n--- First 5 rows of df_invoices ---\")\n",
        "print(df_invoices.head())\n",
        "print(\"\\n--- Null values in df_invoices ---\")\n",
        "print(df_invoices.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_invoices ---\")\n",
        "print(df_invoices.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_invoices ---\")\n",
        "for col in df_invoices.columns:\n",
        "    print(f\"Column '{col}': {df_invoices[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_invoices`:**\n",
        "* Contains details of each invoice, including consumption and relevant dates.\n",
        "* `Invoice Number` is the identifier for the invoice, and `Contract Number` links it to the client.\n",
        "* `Consumption (m3)` is a key numerical variable.\n",
        "* The date columns are also object types."
      ],
      "metadata": {
        "id": "obs_facturas_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Collections DataFrame (`df_collections`)"
      ],
      "metadata": {
        "id": "inspect_recaudo_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_recaudo_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_collections Information ---\")\n",
        "df_collections.info()\n",
        "print(\"\\n--- First 5 rows of df_collections ---\")\n",
        "print(df_collections.head())\n",
        "print(\"\\n--- Null values in df_collections ---\")\n",
        "print(df_collections.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_collections ---\")\n",
        "print(df_collections.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_collections ---\")\n",
        "for col in df_collections.columns:\n",
        "    print(f\"Column '{col}': {df_collections[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_collections`:**\n",
        "* Records the date on which an invoice was paid.\n",
        "* Joins with `df_invoices` using `Invoice Number`.\n",
        "* `Actual Payment Date` will need conversion to `datetime`."
      ],
      "metadata": {
        "id": "obs_recaudo_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Gas Prices DataFrame (`df_gas_prices`)"
      ],
      "metadata": {
        "id": "inspect_precios_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inspect_precios_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- df_gas_prices Information ---\")\n",
        "df_gas_prices.info()\n",
        "print(\"\\n--- First 5 rows of df_gas_prices ---\")\n",
        "print(df_gas_prices.head())\n",
        "print(\"\\n--- Null values in df_gas_prices ---\")\n",
        "print(df_gas_prices.isnull().sum())\n",
        "print(\"\\n--- Descriptive statistics of df_gas_prices ---\")\n",
        "print(df_gas_prices.describe(include='all'))\n",
        "print(\"\\n--- Count of unique values per column in df_gas_prices ---\")\n",
        "for col in df_gas_prices.columns:\n",
        "    print(f\"Column '{col}': {df_gas_prices[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on `df_gas_prices`:**\n",
        "* Defines the price per mÂ³ of gas according to `Year`, `Month`, and `Status`.\n",
        "* `Price m3 (COP)` is the numerical price value.\n",
        "* `Status` is categorical."
      ],
      "metadata": {
        "id": "obs_precios_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3: Merging the Data\n",
        "\n",
        "To get a consolidated view, we will combine these DataFrames."
      ],
      "metadata": {
        "id": "merge_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Joining Invoices with Customer Information\n",
        "\n",
        "We join `df_invoices` with `df_clients` using `Contract Number` as the key."
      ],
      "metadata": {
        "id": "merge_factura_cliente_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_factura_cliente_code"
      },
      "outputs": [],
      "source": [
        "df_invoice_client = pd.merge(df_invoices, df_clients, on='Contract Number', how='left')\n",
        "print(\"--- df_invoice_client Information (Invoices + Clients) ---\")\n",
        "df_invoice_client.info()\n",
        "print(\"\\n--- First 5 rows of df_invoice_client ---\")\n",
        "print(df_invoice_client.head())\n",
        "print(f\"\\nShape of df_invoices: {df_invoices.shape}\")\n",
        "print(f\"Shape of df_invoice_client: {df_invoice_client.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verification:** The number of rows should be equal to that of `df_invoices` if each invoice has a corresponding customer (using `how='left'`). If it increases, it could indicate duplicates in `df_clients` by `Contract Number`, which should not happen."
      ],
      "metadata": {
        "id": "verify_merge1_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Adding Prices to Invoices\n",
        "\n",
        "Now we join `df_invoice_client` with `df_gas_prices`. The join is done using `Year`, `Month`, and the customer's socioeconomic status."
      ],
      "metadata": {
        "id": "merge_precios_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_precios_code"
      },
      "outputs": [],
      "source": [
        "df_invoice_client_price = pd.merge(df_invoice_client,\n",
        "                                     df_gas_prices,\n",
        "                                     left_on=['Year', 'Month', 'Socioeconomic Status'],\n",
        "                                     right_on=['Year', 'Month', 'Status'],\n",
        "                                     how='left')\n",
        "\n",
        "print(\"--- df_invoice_client_price Information (Invoices + Clients + Prices) ---\")\n",
        "df_invoice_client_price.info()\n",
        "print(\"\\n--- First 5 rows of df_invoice_client_price ---\")\n",
        "print(df_invoice_client_price.head())\n",
        "print(f\"\\nShape of df_invoice_client: {df_invoice_client.shape}\")\n",
        "print(f\"Shape of df_invoice_client_price: {df_invoice_client_price.shape}\")\n",
        "\n",
        "# Check for rows where the price could not be assigned (NaN in 'Price m3 (COP)')\n",
        "print(\"\\n--- Rows without an assigned price ---\")\n",
        "print(df_invoice_client_price[df_invoice_client_price['Price m3 (COP)'].isnull()].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verification and Potential Issues:**\n",
        "* Again, the number of rows should be consistent. If it increases drastically, it might indicate that the join keys `['Year', 'Month', 'Socioeconomic Status']` are not unique in `df_gas_prices` for a given combination, or that there are multiple invoices for the same client in the same month with the same status being mapped (which is expected).\n",
        "* The `Status` column from `df_gas_prices` is redundant after the merge and can be dropped.\n",
        "* It's crucial to check if there are any invoices to which a price could not be assigned. This could happen if a combination of `Year, Month, Socioeconomic Status` from the invoices does not exist in the price table."
      ],
      "metadata": {
        "id": "verify_merge2_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Incorporating Payment Information (Collections)\n",
        "\n",
        "Finally, we join the payment information from `df_collections`."
      ],
      "metadata": {
        "id": "merge_recaudo_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_recaudo_code"
      },
      "outputs": [],
      "source": [
        "df_complete = pd.merge(df_invoice_client_price,\n",
        "                       df_collections,\n",
        "                       on='Invoice Number',\n",
        "                       how='left') # We use a left join to keep all invoices, even if they don't have a recorded payment\n",
        "\n",
        "print(\"--- df_complete Information (All data joined) ---\")\n",
        "df_complete.info()\n",
        "print(\"\\n--- First 5 rows of df_complete ---\")\n",
        "print(df_complete.head())\n",
        "print(f\"\\nShape of df_invoice_client_price: {df_invoice_client_price.shape}\")\n",
        "print(f\"Shape of df_complete: {df_complete.shape}\")\n",
        "\n",
        "# Check for invoices without an actual payment date\n",
        "print(\"\\n--- Invoices without actual payment date (NaN in 'Actual Payment Date') ---\")\n",
        "print(df_complete[df_complete['Actual Payment Date'].isnull()][['Invoice Number', 'Issue Date', 'Due Date']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verification:**\n",
        "* The number of rows should remain the same or increase if an invoice has multiple payment records (uncommon, unless they are non-consolidated partial payments). A `left join` ensures all invoices are kept.\n",
        "* Invoices without a corresponding `Actual Payment Date` will have `NaN` in that column. This is expected for unpaid invoices or those whose payment has not yet been recorded."
      ],
      "metadata": {
        "id": "verify_merge3_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Column Selection and Handling Duplicates\n",
        "\n",
        "The final analysis DataFrame will use a subset of columns. We will select these columns and then handle any potential duplicates that may have been generated during the merges, especially if the keys were not perfectly unique or if the same information was joined multiple times."
      ],
      "metadata": {
        "id": "select_cols_dups_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "select_cols_dups_code"
      },
      "outputs": [],
      "source": [
        "selected_columns = [\n",
        "    'Invoice Number', 'Contract Number', 'Issue Date', 'Consumption (m3)',\n",
        "    'Due Date', 'Reading Date', 'Estimated Suspension Date',\n",
        "    'City', 'Socioeconomic Status',\n",
        "    'Price m3 (COP)', 'Actual Payment Date'\n",
        "]\n",
        "\n",
        "if 'Status' in df_complete.columns and 'Socioeconomic Status' in df_complete.columns:\n",
        "    df_complete = df_complete.drop(columns=['Status'])\n",
        "\n",
        "# Create the analysis DataFrame with selected columns\n",
        "df_analysis = df_complete[selected_columns].copy()\n",
        "\n",
        "print(\"--- First rows of df_analysis (before handling duplicates) ---\")\n",
        "print(df_analysis.head())\n",
        "print(f\"\\nShape of df_analysis before drop_duplicates: {df_analysis.shape}\")\n",
        "\n",
        "num_duplicates_before = df_analysis.duplicated().sum()\n",
        "print(f\"Number of exact duplicate rows before: {num_duplicates_before}\")\n",
        "\n",
        "df_analysis = df_analysis.drop_duplicates()\n",
        "print(f\"\\nShape of df_analysis after drop_duplicates: {df_analysis.shape}\")\n",
        "num_duplicates_after = df_analysis.duplicated().sum()\n",
        "print(f\"Number of exact duplicate rows after: {num_duplicates_after}\")\n",
        "\n",
        "print(\"\\n--- First rows of df_analysis (after handling duplicates) ---\")\n",
        "print(df_analysis.head())\n",
        "df_analysis.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note on Duplicates:**\n",
        "The `drop_duplicates()` step is crucial to ensure the integrity of the analysis. It's important to investigate *why* duplicates were generated (e.g., non-unique merge keys? source data with duplicates?). For now, we have removed them."
      ],
      "metadata": {
        "id": "note_duplicates_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 4: Feature Engineering\n",
        "\n",
        "We will create new columns from existing ones to enrich our analysis."
      ],
      "metadata": {
        "id": "feature_eng_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Date Conversion\n",
        "\n",
        "We convert the date columns to `datetime` format."
      ],
      "metadata": {
        "id": "convert_dates_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_dates_code"
      },
      "outputs": [],
      "source": [
        "date_cols = ['Issue Date', 'Due Date', 'Reading Date', 'Actual Payment Date', 'Estimated Suspension Date']\n",
        "for col in date_cols:\n",
        "    df_analysis[col] = pd.to_datetime(df_analysis[col], errors='coerce')\n",
        "\n",
        "print(\"--- Data types after converting dates ---\")\n",
        "df_analysis.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Calculating the Invoice Amount (Price per Consumption)"
      ],
      "metadata": {
        "id": "calc_precio_consumo_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calc_precio_consumo_code"
      },
      "outputs": [],
      "source": [
        "df_analysis['Price per Consumption'] = df_analysis['Price m3 (COP)'] * df_analysis['Consumption (m3)']\n",
        "print(\"\\n--- df_analysis with 'Price per Consumption' ---\")\n",
        "print(df_analysis[['Consumption (m3)', 'Price m3 (COP)', 'Price per Consumption']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Calculating Time Differences\n",
        "\n",
        "We will calculate the number of days between different key events."
      ],
      "metadata": {
        "id": "calc_dias_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calc_dias_code"
      },
      "outputs": [],
      "source": [
        "df_analysis['Days_Issue_DueDate'] = (df_analysis['Due Date'] - df_analysis['Issue Date']).dt.days\n",
        "df_analysis['Days_Reading_Issue'] = (df_analysis['Issue Date'] - df_analysis['Reading Date']).dt.days\n",
        "df_analysis['Days_DueDate_ActualPayment'] = (df_analysis['Actual Payment Date'] - df_analysis['Due Date']).dt.days\n",
        "\n",
        "print(\"\\n--- df_analysis with new days features ---\")\n",
        "print(df_analysis[['Issue Date', 'Due Date', 'Reading Date', 'Actual Payment Date',\n",
        "                   'Days_Issue_DueDate', 'Days_Reading_Issue', 'Days_DueDate_ActualPayment']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Identifying Late Payments\n",
        "\n",
        "We create a binary column `Late Payment`: 1 if the payment was made after the due date, 0 if it was paid on time or before. It is considered 0 if there is no actual payment date (NaT)."
      ],
      "metadata": {
        "id": "calc_mora_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calc_mora_code"
      },
      "outputs": [],
      "source": [
        "df_analysis['Late Payment'] = 0\n",
        "df_analysis.loc[df_analysis['Days_DueDate_ActualPayment'] > 0, 'Late Payment'] = 1\n",
        "df_analysis.loc[df_analysis['Days_DueDate_ActualPayment'].isnull(), 'Late Payment'] = 0 # Assume not late if not paid yet\n",
        "\n",
        "print(\"\\n--- df_analysis with 'Late Payment' column ---\")\n",
        "print(df_analysis[['Due Date', 'Actual Payment Date', 'Days_DueDate_ActualPayment', 'Late Payment']].head(10))\n",
        "print(\"\\nValue counts in 'Late Payment':\")\n",
        "print(df_analysis['Late Payment'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 5: Detailed Exploration of the Consolidated DataFrame (`df_analysis`)\n",
        "\n",
        "Now that we have a clean and enriched DataFrame, we can explore it in more depth."
      ],
      "metadata": {
        "id": "eda_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 General Summary and Null Values"
      ],
      "metadata": {
        "id": "eda_summary_nulls_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eda_summary_nulls_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- General information of df_analysis ---\")\n",
        "df_analysis.info()\n",
        "\n",
        "print(\"\\n--- Null values in df_analysis ---\")\n",
        "print(df_analysis.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Descriptive statistics of df_analysis ---\")\n",
        "print(df_analysis.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on null values:**\n",
        "* `Estimated Suspension Date`: May have nulls if not all invoices have this date.\n",
        "* `Price m3 (COP)` and `Price per Consumption`: Nulls here would indicate problems in the merge with `df_gas_prices` or missing data in the price table.\n",
        "* `Actual Payment Date` and `Days_DueDate_ActualPayment`: Nulls are expected for unpaid invoices.\n",
        "\n",
        "It's important to decide how to handle these nulls. For some variables, they could be imputed; for others (like `Actual Payment Date`), their absence is informative."
      ],
      "metadata": {
        "id": "obs_nulls_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Distribution of Key Numerical Variables"
      ],
      "metadata": {
        "id": "dist_numeric_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dist_numeric_code"
      },
      "outputs": [],
      "source": [
        "numerical_cols_to_plot = ['Consumption (m3)', 'Price m3 (COP)', 'Price per Consumption',\n",
        "                           'Days_Issue_DueDate', 'Days_Reading_Issue', 'Days_DueDate_ActualPayment']\n",
        "\n",
        "for col in numerical_cols_to_plot:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df_analysis[col].dropna(), kde=True, bins=30) # dropna() to avoid errors with NaT/NaN in histplot\n",
        "    plt.title(f'Distribution of {col}')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(y=df_analysis[col].dropna())\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Statistics for {col}:\\n{df_analysis[col].describe()}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of distributions:**\n",
        "* **Consumption (m3):** Observe the shape of the distribution. Is it symmetrical, skewed? Are there outliers (very high or low consumption)?\n",
        "* **Price m3 (COP):** Does it vary much? This will depend on the socioeconomic strata and temporal evolution.\n",
        "* **Price per Consumption:** Similar to consumption, but scaled by price.\n",
        "* **Days Variables:**\n",
        "    * `Days_Issue_DueDate`: The time allowed for payment. Is it constant?\n",
        "    * `Days_Reading_Issue`: Time between meter reading and invoice issuance. Is it consistent?\n",
        "    * `Days_DueDate_ActualPayment`: Positive values indicate late payment. Negative values, early payment. Zero, payment on the due date. The boxplot can show the magnitude of the delay or advance."
      ],
      "metadata": {
        "id": "interpret_dist_numeric_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Distribution of Key Categorical Variables"
      ],
      "metadata": {
        "id": "dist_categorical_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dist_categorical_code"
      },
      "outputs": [],
      "source": [
        "categorical_cols_to_plot = ['City', 'Socioeconomic Status', 'Late Payment']\n",
        "\n",
        "for col in categorical_cols_to_plot:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(data=df_analysis, x=col, order=df_analysis[col].value_counts(dropna=False).index)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "    print(f\"Counts for {col}:\\n{df_analysis[col].value_counts(dropna=False)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of categorical distributions:**\n",
        "* **City:** How are the invoices/customers distributed by city?\n",
        "* **Socioeconomic Status:** Which strata are the most common?\n",
        "* **Late Payment:** What proportion of invoices are paid late?"
      ],
      "metadata": {
        "id": "interpret_dist_categorical_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Correlation Analysis (Numerical Variables)"
      ],
      "metadata": {
        "id": "correlation_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation_code"
      },
      "outputs": [],
      "source": [
        "numerical_df = df_analysis.select_dtypes(include=['number'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_matrix = numerical_df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix of Numerical Variables')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Pairs with highest (absolute) correlation ---\")\n",
        "corr_pairs = correlation_matrix.unstack()\n",
        "sorted_pairs = corr_pairs.sort_values(kind=\"quicksort\", ascending=False)\n",
        "# Filter for relevance (> 0.5) and remove self-correlation and duplicates\n",
        "unique_corr_pairs = sorted_pairs[(abs(sorted_pairs) < 1) & (abs(sorted_pairs) > 0.5)].drop_duplicates()\n",
        "print(unique_corr_pairs.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of the Correlation Matrix:**\n",
        "* Look for correlation coefficients close to 1 (strong positive correlation) or -1 (strong negative correlation).\n",
        "* For example, a high correlation is expected between `Consumption (m3)` and `Price per Consumption`.\n",
        "* Unexpected correlations can reveal interesting insights."
      ],
      "metadata": {
        "id": "interpret_correlation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 Relationships between Variables\n",
        "\n",
        "Let's explore some specific relationships."
      ],
      "metadata": {
        "id": "relationships_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.5.1 Average Consumption by Status and City"
      ],
      "metadata": {
        "id": "consumo_estrato_ciudad_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "consumo_estrato_ciudad_code"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_analysis, x='Socioeconomic Status', y='Consumption (m3)', hue='City', estimator=pd.Series.mean, errorbar=None, order=sorted(df_analysis['Socioeconomic Status'].dropna().unique()))\n",
        "plt.title('Average Consumption (m3) by Socioeconomic Status and City')\n",
        "plt.ylabel('Average Consumption (m3)')\n",
        "plt.xlabel('Socioeconomic Status')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='City')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.boxplot(data=df_analysis, x='Socioeconomic Status', y='Consumption (m3)', hue='City', order=sorted(df_analysis['Socioeconomic Status'].dropna().unique()))\n",
        "plt.title('Distribution of Consumption (m3) by Socioeconomic Status and City')\n",
        "plt.ylabel('Consumption (m3)')\n",
        "plt.xlabel('Socioeconomic Status')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='City')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.5.2 Late Payment Rate by Status and City"
      ],
      "metadata": {
        "id": "mora_estrato_ciudad_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mora_estrato_ciudad_code"
      },
      "outputs": [],
      "source": [
        "late_payment_by_status_city = df_analysis.groupby(['Socioeconomic Status', 'City'])['Late Payment'].mean().reset_index()\n",
        "late_payment_by_status_city = late_payment_by_status_city.rename(columns={'Late Payment': 'Late Payment Rate'})\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=late_payment_by_status_city, x='Socioeconomic Status', y='Late Payment Rate', hue='City', order=sorted(late_payment_by_status_city['Socioeconomic Status'].dropna().unique()))\n",
        "plt.title('Late Payment Rate by Socioeconomic Status and City')\n",
        "plt.ylabel('Late Payment Rate (Proportion)')\n",
        "plt.xlabel('Socioeconomic Status')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='City')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6 Temporal Analysis (Example: Consumption over time)"
      ],
      "metadata": {
        "id": "temporal_analysis_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "temporal_analysis_code"
      },
      "outputs": [],
      "source": [
        "df_temporal = df_analysis.set_index('Issue Date').sort_index()\n",
        "\n",
        "monthly_consumption = df_temporal['Consumption (m3)'].resample('ME').mean()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "monthly_consumption.plot(marker='o', linestyle='-')\n",
        "plt.title('Average Monthly Gas Consumption (m3) Over Time')\n",
        "plt.xlabel('Issue Date (Month)')\n",
        "plt.ylabel('Average Consumption (m3)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "monthly_invoices = df_temporal.resample('ME').size()\n",
        "plt.figure(figsize=(14, 7))\n",
        "monthly_invoices.plot(kind='bar')\n",
        "plt.title('Number of Invoices Issued per Month')\n",
        "plt.xlabel('Issue Date (Month)')\n",
        "plt.ylabel('Number of Invoices')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Temporal Analysis:**\n",
        "* Are there trends in consumption (increase, decrease)?\n",
        "* Is seasonality observed (peaks in certain months)?\n",
        "* The number of invoices can indicate growth in the customer base or fluctuations in billing."
      ],
      "metadata": {
        "id": "interpret_temporal_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 6: Exploration Conclusions and Next Steps\n",
        "\n",
        "In this exploratory phase, we have:\n",
        "1.  Loaded and performed an initial inspection of four data sources.\n",
        "2.  Combined the data into a unified DataFrame (`df_analysis`).\n",
        "3.  Identified and handled duplicate rows that arose during merges.\n",
        "4.  Performed feature engineering, creating new variables like `Price per Consumption`, time differences, and a `Late Payment` indicator.\n",
        "5.  Explored the distributions of key numerical and categorical variables.\n",
        "6.  Analyzed correlations and relationships between variables (e.g., consumption by status, late payment rate by status).\n",
        "7.  Conducted a brief temporal analysis of consumption."
      ],
      "metadata": {
        "id": "conclusions_markdown"
      }
    }
  ]
}