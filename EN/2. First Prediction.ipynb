{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srJboca/segmentacion/blob/main/EN/2.%20First%20prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Predicting Late Payments on Gas Invoices\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook is a continuation of the data exploration tutorial. Here, we will use the `df_analisis.parquet` DataFrame (previously cleaned and enriched) to build a Machine Learning model capable of predicting whether an invoice will be paid late (i.e., if it will be paid after the due date).\n",
        "\n",
        "**Objective:** Predict the `Mora` variable.\n",
        "\n",
        "**Steps we will follow:**\n",
        "1.  Load data and libraries.\n",
        "2.  Perform a quick review and final data preparation for modeling.\n",
        "3.  Select features and the target variable.\n",
        "4.  Encode categorical variables.\n",
        "5.  Split data into training and testing sets.\n",
        "6.  Train a Classification model (Random Forest).\n",
        "7.  Evaluate the model (Accuracy, Classification Report, Confusion Matrix).\n",
        "8.  Analyze feature importance.\n",
        "9.  Discuss results and next steps."
      ],
      "metadata": {
        "id": "intro_prediction_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup and Data Loading"
      ],
      "metadata": {
        "id": "setup_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Importing Libraries"
      ],
      "metadata": {
        "id": "import_libs_pred_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs_pred_code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder # Although we'll prefer manual encoding for Estrato\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Downloading and Loading the Preprocessed DataFrame\n",
        "\n",
        "We will use the `df_analisis.parquet` file, which was the result of the data exploration and preparation notebook."
      ],
      "metadata": {
        "id": "load_preprocessed_data_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_preprocessed_data_code"
      },
      "outputs": [],
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/df_analisis.parquet\n",
        "df_analysis_original = pd.read_parquet('df_analisis.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Quick Review and Final Data Preparation"
      ],
      "metadata": {
        "id": "data_review_prep_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_review_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- First 5 rows of df_analysis_original ---\")\n",
        "print(df_analysis_original.head())\n",
        "print(\"\\n--- Information of df_analysis_original ---\")\n",
        "df_analysis_original.info()\n",
        "print(\"\\n--- Present columns ---\")\n",
        "print(df_analysis_original.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "* The loaded DataFrame (`df_analisis.parquet`) might have two columns related to late payments: `mora` (lowercase) and `Mora` (uppercase). This is an artifact of how the file was saved in the previous notebook. The `Mora` (uppercase) column was the one we created with the defined logic (1 if `Dias_PagoOportuno_PagoReal > 0`, 0 otherwise).\n",
        "* We will ensure we use the correct `Mora` column and remove the redundant one if it exists."
      ],
      "metadata": {
        "id": "obs_mora_column_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "handle_mora_column_code"
      },
      "outputs": [],
      "source": [
        "df_modeling = df_analysis_original.copy()\n",
        "\n",
        "# Check if both 'mora' and 'Mora' columns exist\n",
        "if 'mora' in df_modeling.columns and 'Mora' in df_modeling.columns:\n",
        "    print(\"Both 'mora' and 'Mora' columns exist. Will proceed to use 'Mora' and drop 'mora'.\")\n",
        "    # Before dropping, we could verify if they are identical or which one is correct\n",
        "    # We assume 'Mora' (uppercase) is the one intentionally calculated.\n",
        "    df_modeling = df_modeling.drop(columns=['mora'])\n",
        "elif 'mora' in df_modeling.columns and 'Mora' not in df_modeling.columns:\n",
        "    print(\"Only the 'mora' column exists. It will be renamed to 'Mora'.\")\n",
        "    df_modeling = df_modeling.rename(columns={'mora': 'Mora'})\n",
        "\n",
        "print(\"\\n--- Columns after handling 'mora'/'Mora' ---\")\n",
        "print(df_modeling.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature and Target Variable Selection"
      ],
      "metadata": {
        "id": "feature_selection_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will select the features that we will use to predict the `Mora` variable (target).\n",
        "\n",
        "**Important Note on Data Leakage:**\n",
        "The `Dias_PagoOportuno_PagoReal` column is calculated using `Fecha de Pago Real` and `Fecha de Pago Oportuno`. The `Mora` variable is defined directly from the sign of `Dias_PagoOportuno_PagoReal`. Therefore, **we must NOT use `Dias_PagoOportuno_PagoReal` as a feature** to predict `Mora`, as this would constitute data leakage and the model would learn a trivial relationship, showing artificially perfect performance.\n",
        "\n",
        "Initial candidate features from the original notebook:\n",
        "`Consumo (m3)`, `Estrato`, `Precio por Consumo`, `Dias_Emision_PagoOportuno`, `Dias_Lectura_Emision`."
      ],
      "metadata": {
        "id": "data_leakage_warning_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_selection_code"
      },
      "outputs": [],
      "source": [
        "# Columns for the model, excluding identifiers and the column that causes leakage\n",
        "selected_features = [\n",
        "    'Consumo (m3)',\n",
        "    'Estrato', # This is the original socioeconomic stratum column.\n",
        "    'Precio por Consumo',\n",
        "    'Dias_Emision_PagoOportuno',\n",
        "    'Dias_Lectura_Emision'\n",
        "]\n",
        "target = 'Mora'\n",
        "\n",
        "# Ensure that 'Estrato socioeconomico' is named 'Estrato' if necessary\n",
        "if 'Estrato socioeconomico' in df_modeling.columns and 'Estrato' not in selected_features:\n",
        "    if 'Estrato' not in df_modeling.columns: # Only rename if 'Estrato' doesn't exist\n",
        "         df_modeling = df_modeling.rename(columns={'Estrato socioeconomico': 'Estrato'})\n",
        "    elif 'Estrato' in df_modeling.columns and 'Estrato socioeconomico' in df_modeling.columns:\n",
        "         # If both exist, and 'Estrato' is from df_gas_prices, use 'Estrato socioeconomico'\n",
        "         # and ensure that 'Estrato' in selected_features refers to the correct one.\n",
        "         # For simplicity, we assume that df_analisis.parquet already has the correct 'Estrato' column (the socioeconomic one).\n",
        "         pass # Assume 'Estrato' is already the correct socioeconomic column.\n",
        "\n",
        "df_prediction = df_modeling[selected_features + [target]].copy()\n",
        "\n",
        "print(\"--- DataFrame for prediction (df_prediction) ---\")\n",
        "print(df_prediction.head())\n",
        "print(\"\\n--- Data types in df_prediction ---\")\n",
        "df_prediction.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing for the Model"
      ],
      "metadata": {
        "id": "preprocessing_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Encoding the 'Estrato' Variable\n",
        "\n",
        "The `Estrato` variable is categorical (e.g., 'Estrato 1', 'Estrato 2'). Machine Learning models require numerical inputs. We will convert 'Estrato' to an ordinal numerical type, as the strata have an inherent order."
      ],
      "metadata": {
        "id": "encode_estrato_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "encode_estrato_code"
      },
      "outputs": [],
      "source": [
        "if df_prediction['Estrato'].dtype == 'object' or isinstance(df_prediction['Estrato'].dtype, pd.CategoricalDtype):\n",
        "    print(\"Encoding 'Estrato' from object/categorical to numeric type.\")\n",
        "    try:\n",
        "        df_prediction['Estrato'] = df_prediction['Estrato'].str.replace('Estrato ', '', regex=False).astype(int)\n",
        "    except AttributeError:\n",
        "        # If it's already numeric (e.g., if the parquet has it as a number but read as category)\n",
        "        df_prediction['Estrato'] = df_prediction['Estrato'].astype(str).str.replace('Estrato ', '', regex=False).astype(int)\n",
        "else:\n",
        "    print(\"'Estrato' is already numeric.\")\n",
        "    df_prediction['Estrato'] = df_prediction['Estrato'].astype(int) # Ensure it is int\n",
        "\n",
        "print(\"\\n--- 'Estrato' after encoding ---\")\n",
        "print(df_prediction[['Estrato']].head())\n",
        "print(df_prediction['Estrato'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Handling Missing Values (NaN)\n",
        "\n",
        "We check for missing values in the selected features. For this tutorial, if there are only a few, we might drop them or use simple imputation. Models like RandomForest can handle NaNs in some implementations, but it's good practice to address them."
      ],
      "metadata": {
        "id": "handle_nan_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "handle_nan_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- Missing values in df_prediction before handling ---\")\n",
        "print(df_prediction.isnull().sum())\n",
        "\n",
        "# Simple strategy: drop rows with NaNs in features or the target\n",
        "# (Consider imputation for a more complex real-world case)\n",
        "df_prediction_final = df_prediction.dropna()\n",
        "\n",
        "print(f\"\\nOriginal shape: {df_prediction.shape}\")\n",
        "print(f\"Shape after dropna: {df_prediction_final.shape}\")\n",
        "print(\"\\n--- Missing values in df_prediction_final after handling ---\")\n",
        "print(df_prediction_final.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Defining X (Features) and y (Target Variable)"
      ],
      "metadata": {
        "id": "define_X_y_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_X_y_code"
      },
      "outputs": [],
      "source": [
        "X = df_prediction_final.drop(target, axis=1)\n",
        "y = df_prediction_final[target]\n",
        "\n",
        "print(\"--- Features (X) ---\")\n",
        "print(X.head())\n",
        "print(\"\\n--- Target Variable (y) ---\")\n",
        "print(y.head())\n",
        "print(\"\\nDistribution of the target variable 'Mora':\")\n",
        "print(y.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the target variable `Mora` tells us if there is a class imbalance. If one class is much more frequent than the other, we might need special techniques (like `class_weight='balanced'` in the model or over/under-sampling)."
      ],
      "metadata": {
        "id": "class_imbalance_note_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Splitting Data: Training and Testing"
      ],
      "metadata": {
        "id": "train_test_split_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the data into a training set (for the model to learn from) and a testing set (to evaluate its performance on unseen data).\n",
        "We use `stratify=y` to ensure that the proportion of classes in `Mora` is similar in both sets."
      ],
      "metadata": {
        "id": "explain_tts_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_test_split_code"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Size of X_train: {X_train.shape}\")\n",
        "print(f\"Size of X_test: {X_test.shape}\")\n",
        "print(f\"Size of y_train: {y_train.shape}\")\n",
        "print(f\"Size of y_test: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nProportion of 'Mora' in y_train:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"\\nProportion of 'Mora' in y_test:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training the Model (Random Forest Classifier)"
      ],
      "metadata": {
        "id": "model_training_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a `RandomForestClassifier`. It's a robust and popular ensemble model.\n",
        "The `class_weight='balanced'` parameter helps the model treat the classes more equally if there is an imbalance."
      ],
      "metadata": {
        "id": "explain_rf_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_training_code"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"RandomForestClassifier model trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluating the Model"
      ],
      "metadata": {
        "id": "model_evaluation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will evaluate the model on the test set using several metrics:"
      ],
      "metadata": {
        "id": "eval_intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_evaluation_code"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"--- Evaluation Results ---\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Visualizing the Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Late (0)', 'Late (1)'], yticklabels=['Not Late (0)', 'Late (1)'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the Metrics:**\n",
        "* **Accuracy:** Proportion of correct predictions. Can be misleading if classes are imbalanced.\n",
        "* **Classification Report:**\n",
        "    * **Precision:** Of all the predictions for a class, how many were correct? (TP / (TP + FP)). Important if the cost of a False Positive is high.\n",
        "    * **Recall:** Of all the actual instances of a class, how many were correctly identified? (TP / (TP + FN)). Important if the cost of a False Negative is high (e.g., failing to detect an invoice that will be paid late).\n",
        "    * **F1-score:** Harmonic mean of Precision and Recall. A good overall indicator of performance, especially with imbalanced classes.\n",
        "    * **Support:** The number of actual instances of each class.\n",
        "* **Confusion Matrix:**\n",
        "    * **True Negatives (TN):** 'Not Late' cases correctly predicted as 'Not Late'.\n",
        "    * **False Positives (FP):** 'Not Late' cases incorrectly predicted as 'Late' (Type I Error).\n",
        "    * **False Negatives (FN):** 'Late' cases incorrectly predicted as 'Not Late' (Type II Error).\n",
        "    * **True Positives (TP):** 'Late' cases correctly predicted as 'Late'."
      ],
      "metadata": {
        "id": "metrics_interpretation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Feature Importance"
      ],
      "metadata": {
        "id": "feature_importance_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest allows us to see which features were most influential in the prediction."
      ],
      "metadata": {
        "id": "explain_feat_imp_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_importance_code"
      },
      "outputs": [],
      "source": [
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
        "plt.title('Feature Importance for Predicting Late Payment')\n",
        "plt.show()\n",
        "\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting Feature Importance:**\n",
        "This shows us which factors the model considers most decisive in determining whether an invoice will be paid late. It can guide business decisions or future refinements of the model."
      ],
      "metadata": {
        "id": "interpret_feat_imp_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Discussion of Results and Next Steps\n",
        "\n",
        "**Questions for Discussion:**\n",
        "* Is the accuracy good? How are precision/recall for the 'Mora=1' class? Which features are the most important?\n",
        "* Is the model (Random Forest) good, average, or poor for predicting late payments under the current conditions and with the selected features?\n",
        "* It's crucial to remember that data leakage was avoided by not including `Dias_PagoOportuno_PagoReal`.\n",
        "\n",
        "**Limitations and Considerations:**\n",
        "* **Data Quality:** The model's performance depends on the quality and representativeness of the input data.\n",
        "* **Feature Engineering:** More features could be explored (e.g., customer payment history, variations in consumption, etc.).\n",
        "* **Class Balance:** If the 'Mora=1' class is a minority, accuracy may not be the best metric. F1-score, recall for the minority class, or AUC-ROC (not calculated here) are more informative.\n",
        "* **Simplification:** A simple strategy for handling NaNs was used. More sophisticated methods might be necessary.\n",
        "\n",
        "**Potential Next Steps:**\n",
        "* **Hyperparameter Tuning:** Use techniques like GridSearchCV or RandomizedSearchCV to find the best combination of hyperparameters for the RandomForest.\n",
        "* **Try Other Models:** Evaluate other algorithms (e.g., Logistic Regression, Gradient Boosting, SVM).\n",
        "* **Advanced Feature Engineering:** Create more complex or domain-specific variables.\n",
        "* **Error Analysis:** Investigate the cases where the model makes mistakes (False Positives and False Negatives) to understand its weaknesses.\n",
        "* **Deployment to Production:** If the model is satisfactory, plan how it would be integrated into a system to make predictions on new invoices.\n",
        "\n",
        "This tutorial provides a foundation for late payment prediction. Modeling is an iterative process of experimentation and improvement."
      ],
      "metadata": {
        "id": "discussion_conclusion_markdown"
      }
    }
  ]
}