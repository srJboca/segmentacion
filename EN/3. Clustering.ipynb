{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srJboca/segmentacion/blob/main/EN/3.%20Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Gas Customer Segmentation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook focuses on customer segmentation using the `df_analisis.parquet` DataFrame, which was prepared in the data exploration notebook. Customer segmentation allows us to group customers with similar characteristics, which is useful for personalized marketing strategies, service management, and operational optimization.\n",
        "\n",
        "**Objective:** Identify distinct customer segments based on their consumption and payment behavior.\n",
        "\n",
        "**Techniques Used:**\n",
        "1.  **Data Aggregation:** Transforming invoice-level data into customer-level features.\n",
        "2.  **Principal Component Analysis (PCA):** Reducing the dimensionality of the data to facilitate visualization and clustering.\n",
        "3.  **K-Means Clustering:** Grouping customers into K distinct segments.\n",
        "4.  **Segment Profiling:** Analyzing the characteristics of each segment.\n",
        "\n",
        "## 1. Environment Setup and Data Loading"
      ],
      "metadata": {
        "id": "intro_segmentation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Importing Libraries"
      ],
      "metadata": {
        "id": "import_libs_seg_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs_seg_code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Downloading and Loading the Preprocessed DataFrame\n",
        "\n",
        "We will use the `df_analisis.parquet` file."
      ],
      "metadata": {
        "id": "load_preprocessed_data_seg_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_preprocessed_data_seg_code"
      },
      "outputs": [],
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/df_analisis.parquet\n",
        "df_analysis = pd.read_parquet('df_analisis.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Quick Data Review\n",
        "Let's recall the structure of the `df_analysis` DataFrame."
      ],
      "metadata": {
        "id": "data_review_seg_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_review_seg_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- First 5 rows of df_analysis ---\")\n",
        "print(df_analysis.head())\n",
        "print(\"\\n--- Information of df_analysis ---\")\n",
        "df_analysis.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Aggregating Features at the Customer Level\n",
        "\n",
        "To segment customers, we need features that describe each customer's behavior over time. We will group the data by `Numero de contrato` and calculate aggregate metrics (averages) of their consumption, costs, and payment behavior."
      ],
      "metadata": {
        "id": "feature_aggregation_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_aggregation_code"
      },
      "outputs": [],
      "source": [
        "df_grouped = df_analysis.groupby('Numero de contrato').agg(\n",
        "    Average_Consumption=('Consumo (m3)', 'mean'),\n",
        "    Average_Consumption_Price=('Precio por Consumo', 'mean'),\n",
        "    Average_Days_Issue_DueDate=('Dias_Emision_PagoOportuno', 'mean'),\n",
        "    Average_Days_Reading_Issue=('Dias_Lectura_Emision', 'mean'),\n",
        "    Average_Days_DueDate_ActualPayment=('Dias_PagoOportuno_PagoReal', 'mean'),\n",
        "    Average_Late_Payment_Rate=('Mora', 'mean') # Late payment rate\n",
        ").reset_index()\n",
        "\n",
        "print(\"--- df_grouped (data aggregated by customer) ---\")\n",
        "print(df_grouped.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Incorporating 'Estrato Socioecon√≥mico'\n",
        "\n",
        "The 'Estrato socioeconomico' is an important customer characteristic. We will join it to the grouped DataFrame. We need to get the stratum from `df_analysis`. Since `df_analysis` already has the 'Estrato' column, we will use it, making sure to take a single value per customer."
      ],
      "metadata": {
        "id": "merge_estrato_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_estrato_code"
      },
      "outputs": [],
      "source": [
        "# Get the stratum for each contract (taking the first one, assuming it doesn't change)\n",
        "df_strata = df_analysis.drop_duplicates(subset=['Numero de contrato'])[['Numero de contrato', 'Estrato']].copy()\n",
        "# Rename 'Estrato' to 'Estrato socioeconomico' for clarity\n",
        "df_strata.rename(columns={'Estrato': 'Estrato socioeconomico'}, inplace=True)\n",
        "\n",
        "df_segmentation = pd.merge(df_grouped, df_strata, on='Numero de contrato', how='left')\n",
        "\n",
        "print(\"--- df_segmentation with Estrato ---\")\n",
        "print(df_segmentation.head())\n",
        "print(\"\\n--- Information about df_segmentation ---\")\n",
        "df_segmentation.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing for PCA and Clustering"
      ],
      "metadata": {
        "id": "pca_preprocessing_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Selecting Numerical Features and Handling NaNs\n",
        "We will select the numerical features for PCA and clustering. The 'Estrato socioeconomico' column is categorical, and we will handle it later or use it to profile the segments. For now, we will convert it to a numerical type since it's an ordinal feature.\n",
        "We will also remove rows with NaN values in the selected features."
      ],
      "metadata": {
        "id": "select_numeric_nan_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "select_numeric_nan_code"
      },
      "outputs": [],
      "source": [
        "# Convert 'Estrato socioeconomico' to numeric (ordinal)\n",
        "if df_segmentation['Estrato socioeconomico'].dtype == 'object' or isinstance(df_segmentation['Estrato socioeconomico'].dtype, pd.CategoricalDtype):\n",
        "    df_segmentation['Stratum_Num'] = df_segmentation['Estrato socioeconomico'].str.replace('Estrato ', '', regex=False).astype(int)\n",
        "else:\n",
        "    df_segmentation['Stratum_Num'] = df_segmentation['Estrato socioeconomico'].astype(int)\n",
        "\n",
        "features_for_pca = [\n",
        "    'Average_Consumption',\n",
        "    'Average_Consumption_Price',\n",
        "    'Average_Days_Issue_DueDate',\n",
        "    'Average_Days_Reading_Issue',\n",
        "    'Average_Days_DueDate_ActualPayment',\n",
        "    'Average_Late_Payment_Rate',\n",
        "    'Stratum_Num' # We include the numerical stratum\n",
        "]\n",
        "X = df_segmentation[features_for_pca].copy()\n",
        "\n",
        "print(f\"Shape before dropna: {X.shape}\")\n",
        "X.dropna(inplace=True) # Remove rows with NaNs in these features\n",
        "print(f\"Shape after dropna: {X.shape}\")\n",
        "\n",
        "print(\"\\nMissing values after dropna:\")\n",
        "print(X.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Feature Scaling\n",
        "PCA is sensitive to the scale of the features. Therefore, we will standardize the data."
      ],
      "metadata": {
        "id": "scaling_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaling_code"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"--- Scaled data (first 5 rows) ---\")\n",
        "print(pd.DataFrame(X_scaled, columns=X.columns).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Principal Component Analysis (PCA)\n",
        "\n",
        "We will reduce the dimensionality to 2 principal components for visualization."
      ],
      "metadata": {
        "id": "pca_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pca_code"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2) # Reduce to 2 components\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "df_pca = pd.DataFrame(data=X_pca, columns=['principal_component_1', 'principal_component_2'])\n",
        "\n",
        "print(\"--- Principal Components (first 5 rows) ---\")\n",
        "print(df_pca.head())\n",
        "\n",
        "print(f\"\\nExplained variance by each component: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance (2 components): {pca.explained_variance_ratio_.sum():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total explained variance tells us what percentage of the original information is retained in the 2 principal components."
      ],
      "metadata": {
        "id": "pca_variance_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. K-Means Clustering"
      ],
      "metadata": {
        "id": "kmeans_intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Elbow Method for Optimal K\n",
        "We will use the elbow method to help determine an appropriate number of clusters (K). We are looking for the point where adding more clusters does not significantly improve the Within-Cluster Sum of Squares (WCSS) or inertia."
      ],
      "metadata": {
        "id": "elbow_method_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elbow_method_code"
      },
      "outputs": [],
      "source": [
        "inertia_list = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto') # n_init='auto' is the default in recent versions\n",
        "    kmeans.fit(df_pca) # We use the PCA-transformed data\n",
        "    inertia_list.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inertia_list, marker='o')\n",
        "plt.title('Elbow Method for Finding Optimal K')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the plot above. The \"elbow\" is the point where the rate of decrease in inertia flattens. This point suggests an optimal K value. For this tutorial, we will use K=4.\n"
      ],
      "metadata": {
        "id": "elbow_interpretation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Applying K-Means and Visualization\n",
        "We apply K-Means with the chosen K (K=4) and visualize the clusters in the PCA space."
      ],
      "metadata": {
        "id": "apply_kmeans_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apply_kmeans_code",
        "colab_both_odeon_parameters": "{\n  \"dic_map\": {\n    \"optimal_k\": 4\n  },\n  \"options\": [\n    {\n      \"type\": \"integer\",\n      \"key\": \"optimal_k\",\n      \"label\": \"Clusters to use\",\n      \"validation\": [\n        {\n          \"min\": 2,\n          \"max\": 10,\n          \"message\": \"The number of clusters must be between 2 and 10\"\n        }\n      ]\n    }\n  ]\n}"
      },
      "outputs": [],
      "source": [
        "optimal_k = 4 # @param {type:\"integer\"} # Chosen from the elbow method or business analysis\n",
        "\n",
        "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
        "df_pca['segment'] = kmeans_optimal.fit_predict(df_pca)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x='principal_component_1', y='principal_component_2', hue='segment', data=df_pca, palette='viridis', s=100, alpha=0.7)\n",
        "plt.title(f'Customer Clusters (k={optimal_k}) in PCA Space')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='Customer Segment')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"--- df_pca with Segments ---\")\n",
        "print(df_pca.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Segment Profiling\n",
        "\n",
        "Now that we have the segments, we need to understand what characterizes each one. We will join the segment labels back to the `df_segmentation` DataFrame (which contains the aggregated features and the stratum) and analyze the mean of the features for each segment."
      ],
      "metadata": {
        "id": "profiling_intro_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add_segment_to_grouped_code"
      },
      "outputs": [],
      "source": [
        "# Since X was created from X.dropna(), df_pca should have the same length and order\n",
        "# We create df_segmentation_cleaned to ensure the index correspondence\n",
        "df_segmentation_cleaned = df_segmentation.loc[X.index].copy()\n",
        "df_segmentation_cleaned['segment'] = df_pca['segment'].values\n",
        "\n",
        "print(\"--- df_segmentation_cleaned with Segments ---\")\n",
        "print(df_segmentation_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Average Characteristics per Segment"
      ],
      "metadata": {
        "id": "average_chars_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "average_chars_code"
      },
      "outputs": [],
      "source": [
        "segment_profiles = df_segmentation_cleaned.groupby('segment')[features_for_pca].mean()\n",
        "print(\"--- Segment Profiles (Average Characteristics) ---\")\n",
        "print(segment_profiles)\n",
        "\n",
        "# Visualizing the profiles\n",
        "segment_profiles.T.plot(kind='bar', figsize=(15, 7))\n",
        "plt.title('Average Characteristics per Customer Segment')\n",
        "plt.ylabel('Average Value')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Segment')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Stratum Distribution per Segment"
      ],
      "metadata": {
        "id": "estrato_dist_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "estrato_dist_code"
      },
      "outputs": [],
      "source": [
        "stratum_segment_distribution = pd.crosstab(df_segmentation_cleaned['segment'], df_segmentation_cleaned['Estrato socioeconomico'], normalize='index') * 100\n",
        "print(\"\\n--- Percentage Distribution of Stratum per Segment ---\")\n",
        "print(stratum_segment_distribution)\n",
        "\n",
        "stratum_segment_distribution.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='viridis')\n",
        "plt.title('Distribution of Socioeconomic Stratum by Segment')\n",
        "plt.xlabel('Customer Segment')\n",
        "plt.ylabel('Percentage of Customers (%)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Stratum')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the Profiles:**\n",
        "* **Average Characteristics:** Look at the bars for each segment. Which characteristics have high or low values in a particular segment? For example, one segment might have a high 'Average_Consumption' and a low 'Average_Late_Payment_Rate', indicating high-value customers with good payment behavior.\n",
        "* **Stratum Distribution:** Is there a predominant stratum in each segment? This can help understand the socioeconomic composition of the groups.\n",
        "\n",
        "**Naming the Segments (Example):**\n",
        "Based on the analysis, you could assign descriptive names to each segment, such as:\n",
        "* **Segment 0:** Low Consumption, Punctual Payers\n",
        "* **Segment 1:** High Consumption, Moderate Late Payments, High Stratum\n",
        "* Etc."
      ],
      "metadata": {
        "id": "profile_interpretation_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Conclusions and Next Steps\n",
        "\n",
        "In this notebook, we have:\n",
        "1.  Aggregated data at the customer level.\n",
        "2.  Used PCA to reduce dimensionality and facilitate visualization.\n",
        "3.  Applied K-Means to group customers into segments (K=4 in this example).\n",
        "4.  Begun profiling the segments by analyzing their average characteristics and stratum distribution.\n",
        "\n",
        "**Potential Next Steps:**\n",
        "* **Segment Validation:** Use clustering metrics (like the Silhouette Coefficient) to evaluate the quality of the segments. Also, validate the stability of the segments with different K-Means initializations or data subsets.\n",
        "* **Detailed Profiling:** Analyze more variables for each segment (e.g., customer tenure, contract type, if they were included).\n",
        "* **Strategic Actions:** Define specific strategies for each segment (e.g., loyalty campaigns for high-value customers, late payment management programs for segments with high default rates, personalized offers based on consumption).\n",
        "* **Monitoring:** Observe how segments evolve over time and whether customers move from one segment to another.\n",
        "* **Try other algorithms:** Explore other clustering algorithms (e.g., DBSCAN, Agglomerative Clustering) if K-Means does not produce satisfactory results or if non-spherical cluster structures are suspected.\n",
        "\n",
        "Customer segmentation is a powerful tool for better understanding the customer base and making more informed business decisions."
      ],
      "metadata": {
        "id": "segmentation_conclusion_markdown"
      }
    }
  ]
}