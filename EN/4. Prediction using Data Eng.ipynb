{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srJboca/segmentacion/blob/main/EN/4.%20Prediction%20with%20PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Late Payment Prediction with PCA\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook will guide you through building a model to predict 'Mora' (payment default) for customers. We will explore the use of Principal Component Analysis (PCA) for dimensionality reduction before training a binary classifier.\n",
        "\n",
        "**Key Points of the Tutorial:**\n",
        "1.  Preparing data for prediction.\n",
        "2.  Applying PCA to reduce the dimensionality of the feature set.\n",
        "3.  Training and evaluating a binary classification model using the principal components.\n",
        "4.  A method for estimating the importance of the original features after applying PCA.\n",
        "\n",
        "This iterative approach is common in the development of machine learning models."
      ],
      "metadata": {
        "id": "-rd1c4pa76AS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs_seg_code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Loading the Preprocessed DataFrame\n",
        "\n",
        "We will use the `df_analisis.parquet` file."
      ],
      "metadata": {
        "id": "load_preprocessed_data_seg_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_preprocessed_data_seg_code"
      },
      "outputs": [],
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/df_analisis.parquet\n",
        "df_analysis = pd.read_parquet('df_analisis.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Quick Data Review\n",
        "Let's recall the structure of the `df_analysis` DataFrame."
      ],
      "metadata": {
        "id": "data_review_seg_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_review_seg_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- First 5 rows of df_analysis ---\")\n",
        "print(df_analysis.head())\n",
        "print(\"\\n--- Information of df_analysis ---\")\n",
        "df_analysis.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_analysis[[\n",
        "    'Numero de factura',\n",
        "    'Consumo (m3)',\n",
        "    'Estrato',\n",
        "    'Precio m3 (COP)',\n",
        "    'Dias_Emision_PagoOportuno',\n",
        "    'Dias_Lectura_Emision',\n",
        "    'Dias_PagoOportuno_PagoReal',\n",
        "    'Mora'\n",
        "]].copy()\n",
        "\n",
        "# Check the first few rows and info of the filtered dataframe\n",
        "print(\"\\n--- First 5 rows of the filtered DataFrame ---\")\n",
        "print(df_filtered.head())\n",
        "print(\"\\n--- Information of the filtered DataFrame ---\")\n",
        "df_filtered.info()"
      ],
      "metadata": {
        "id": "gCSoenGmyPfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "select_numeric_nan_code"
      },
      "outputs": [],
      "source": [
        "# Convert 'Estrato' to numeric (ordinal)\n",
        "\n",
        "if df_filtered['Estrato'].dtype == 'object' or isinstance(df_filtered['Estrato'].dtype, pd.CategoricalDtype):\n",
        "    df_filtered['Stratum_Num'] = df_filtered['Estrato'].str.replace('Estrato ', '', regex=False).astype(int)\n",
        "else:\n",
        "    df_filtered['Stratum_Num'] = df_filtered['Estrato'].astype(int)\n",
        "\n",
        "features_for_pca = [\n",
        "    'Consumo (m3)',\n",
        "    'Stratum_Num',\n",
        "    'Precio m3 (COP)',\n",
        "    'Dias_Emision_PagoOportuno',\n",
        "    'Dias_Lectura_Emision',\n",
        "    'Dias_PagoOportuno_PagoReal',\n",
        "]\n",
        "X = df_filtered[features_for_pca].copy()\n",
        "\n",
        "print(f\"Shape before dropna: {X.shape}\")\n",
        "X.dropna(inplace=True) # Remove rows with NaNs in these features\n",
        "print(f\"Shape after dropna: {X.shape}\")\n",
        "\n",
        "print(\"\\nMissing values after dropna:\")\n",
        "print(X.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Feature Scaling\n",
        "PCA is sensitive to the scale of the features. Therefore, we will standardize the data."
      ],
      "metadata": {
        "id": "scaling_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaling_code"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"--- Scaled data (first 5 rows) ---\")\n",
        "print(pd.DataFrame(X_scaled, columns=X.columns).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Principal Component Analysis (PCA)\n",
        "\n",
        "We will reduce the dimensionality to 2 principal components for visualization."
      ],
      "metadata": {
        "id": "pca_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pca_code"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2) # Reduce to 2 components\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "df_pca = pd.DataFrame(data=X_pca, columns=['principal_component_1', 'principal_component_2'])\n",
        "\n",
        "print(\"--- Principal Components (first 5 rows) ---\")\n",
        "print(df_pca.head())\n",
        "\n",
        "print(f\"\\nExplained variance by each component: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance (2 components): {pca.explained_variance_ratio_.sum():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pca = df_pca.set_index(X.index)\n",
        "df_pca = df_pca.join(df_filtered[['Mora']])\n",
        "\n",
        "print(\"\\n--- df_pca with 'Mora' column (first 5 rows) ---\")\n",
        "print(df_pca.head())\n",
        "\n",
        "print(\"\\n--- df_pca Information ---\")\n",
        "df_pca.info()"
      ],
      "metadata": {
        "id": "wpU7pUQm0a0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='principal_component_1',\n",
        "    y='principal_component_2',\n",
        "    hue='Mora',  # Color the points according to the 'Mora' value (0 or 1)\n",
        "    data=df_pca,\n",
        "    palette='viridis', # Optional: change the color palette\n",
        "    alpha=0.6 # Optional: adjust the transparency of the points\n",
        ")\n",
        "\n",
        "plt.title('PCA Scatter Plot: PCA1 vs PCA2 colored by Mora')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yo6sJYdB0m6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Define features (PCA components) and the target variable\n",
        "X_model = df_pca[['principal_component_1', 'principal_component_2']]\n",
        "y_model = df_pca['Mora']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.3, random_state=42, stratify=y_model) # Stratify to maintain the 'Mora' proportion\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"Proportion of 'Mora' in training set: {y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Proportion of 'Mora' in test set: {y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Optional: Visualize the decision boundaries (only works well for 2 components)\n",
        "# You can run this part to see how the model separates the classes in the PCA space.\n",
        "\n",
        "# Create a grid to plot the decision boundaries\n",
        "x_min, x_max = X_model['principal_component_1'].min() - 0.5, X_model['principal_component_1'].max() + 0.5\n",
        "y_min, y_max = X_model['principal_component_2'].min() - 0.5, X_model['principal_component_2'].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Predict the class for each point in the grid\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundaries and data points\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis') # Background colors for the regions\n",
        "\n",
        "sns.scatterplot(\n",
        "    x='principal_component_1',\n",
        "    y='principal_component_2',\n",
        "    hue='Mora',\n",
        "    data=df_pca,\n",
        "    palette='viridis',\n",
        "    alpha=0.6,\n",
        "    edgecolor='k', # Add border to points for better visibility\n",
        "    s=50 # Adjust point size\n",
        ")\n",
        "\n",
        "plt.title('Classification Model Decision Boundaries with PCA')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "W07gwiGO0mwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Not Late (0)', 'Late (1)'],\n",
        "            yticklabels=['Not Late (0)', 'Late (1)'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Explanation of the Confusion Matrix\n",
        "print(\"\\n--- Explanation of the Confusion Matrix ---\")\n",
        "print(f\"The confusion matrix shows the performance of our model on the test set.\")\n",
        "print(f\"The rows represent the actual classes (Actual), and the columns represent the predicted classes (Predicted).\")\n",
        "print(f\"We have 4 main cells:\")\n",
        "print(f\"  - Top-left (True Negatives, TN): {cm[0, 0]} cases where the actual value was 0 (Not Late) and the model predicted 0 (Not Late).\")\n",
        "print(f\"  - Top-right (False Positives, FP): {cm[0, 1]} cases where the actual value was 0 (Not Late) but the model predicted 1 (Late). These are 'Type I' errors.\")\n",
        "print(f\"  - Bottom-left (False Negatives, FN): {cm[1, 0]} cases where the actual value was 1 (Late) but the model predicted 0 (Not Late). These are 'Type II' errors.\")\n",
        "print(f\"  - Bottom-right (True Positives, TP): {cm[1, 1]} cases where the actual value was 1 (Late) and the model predicted 1 (Late).\")\n",
        "\n",
        "print(f\"\\nFrom these values, metrics are calculated such as:\")\n",
        "print(f\"  - Accuracy = (TN + TP) / Total cases = ({cm[0, 0]} + {cm[1, 1]}) / {np.sum(cm):.0f} = {accuracy:.4f}\")\n",
        "print(f\"  - Precision (for class 1, Late) = TP / (TP + FP) = {cm[1, 1]} / ({cm[1, 1]} + {cm[0, 1]}): Proportion of positive predictions (Late) that were correct.\")\n",
        "print(f\"  - Recall (Sensitivity, for class 1, Late) = TP / (TP + FN) = {cm[1, 1]} / ({cm[1, 1]} + {cm[1, 0]}): Proportion of actual positive cases (Late) that were correctly identified.\")\n",
        "print(f\"  - F1-Score (for class 1, Late): Harmonic mean of Precision and Recall, useful when there is a class imbalance.\")\n",
        "print(f\"These metrics, especially Precision and Recall, give us a more detailed view of the model's performance, particularly in identifying cases of 'Mora', which may be the class of primary interest.\")\n"
      ],
      "metadata": {
        "id": "uKYdJSF29ME8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}