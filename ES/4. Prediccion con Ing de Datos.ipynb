{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srJboca/segmentacion/blob/main/ES/4.%20Prediccion%20con%20Ing%20de%20Datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Predicción de Mora con PCA\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Este notebook te guiará a través de la construcción de un modelo para predecir la 'Mora' (incumplimiento de pago) de los clientes. Exploraremos el uso del Análisis de Componentes Principales (PCA) para la reducción de dimensionalidad antes de entrenar un clasificador binario.\n",
        "\n",
        "**Puntos Clave del Tutorial:**\n",
        "1.  Preparación de datos para la predicción.\n",
        "2.  Aplicación de PCA para reducir la dimensionalidad del conjunto de características.\n",
        "3.  Entrenamiento y evaluación de un modelo de clasificación binaria utilizando los componentes principales.\n",
        "4.  Un método para estimar la importancia de las características originales después de aplicar PCA.\n",
        "\n",
        "Este enfoque iterativo es común en el desarrollo de modelos de machine learning."
      ],
      "metadata": {
        "id": "-rd1c4pa76AS"
      },
      "id": "-rd1c4pa76AS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs_seg_code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "id": "import_libs_seg_code"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descarga y Carga del DataFrame Preprocesado\n",
        "\n",
        "Utilizaremos el archivo `df_analisis.parquet`."
      ],
      "metadata": {
        "id": "load_preprocessed_data_seg_markdown"
      },
      "id": "load_preprocessed_data_seg_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_preprocessed_data_seg_code"
      },
      "outputs": [],
      "source": [
        "!wget -N https://github.com/srJboca/segmentacion/raw/refs/heads/main/archivos/df_analisis.parquet\n",
        "df_analisis = pd.read_parquet('df_analisis.parquet')"
      ],
      "id": "load_preprocessed_data_seg_code"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Revisión Rápida de Datos\n",
        "Recordemos la estructura del DataFrame `df_analisis`."
      ],
      "metadata": {
        "id": "data_review_seg_markdown"
      },
      "id": "data_review_seg_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_review_seg_code"
      },
      "outputs": [],
      "source": [
        "print(\"--- Primeras 5 filas de df_analisis ---\")\n",
        "print(df_analisis.head())\n",
        "print(\"\\n--- Información de df_analisis ---\")\n",
        "df_analisis.info()"
      ],
      "id": "data_review_seg_code"
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtrado = df_analisis[[\n",
        "    'Numero de factura',\n",
        "    'Consumo (m3)',\n",
        "    'Estrato',\n",
        "    'Precio m3 (COP)',\n",
        "    'Dias_Emision_PagoOportuno',\n",
        "    'Dias_Lectura_Emision',\n",
        "    'Dias_PagoOportuno_PagoReal',\n",
        "    'Mora'\n",
        "]].copy()\n",
        "\n",
        "# Verificar las primeras filas y la información del dataframe filtrado\n",
        "print(\"\\n--- Primeras 5 filas del DataFrame filtrado ---\")\n",
        "print(df_filtrado.head())\n",
        "print(\"\\n--- Información del DataFrame filtrado ---\")\n",
        "df_filtrado.info()"
      ],
      "metadata": {
        "id": "gCSoenGmyPfP"
      },
      "id": "gCSoenGmyPfP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "select_numeric_nan_code"
      },
      "outputs": [],
      "source": [
        "# Convertir 'Estrato socioeconomico' a numérico (ordinal)\n",
        "\n",
        "if df_filtrado['Estrato'].dtype == 'object' or isinstance(df_filtrado['Estrato'].dtype, pd.CategoricalDtype):\n",
        "    df_filtrado['Estrato_Num'] = df_filtrado['Estrato'].str.replace('Estrato ', '', regex=False).astype(int)\n",
        "else:\n",
        "    df_filtrado['Estrato_Num'] = df_filtrado['Estrato'].astype(int)\n",
        "\n",
        "features_for_pca = [\n",
        "    'Consumo (m3)',\n",
        "    'Estrato_Num',\n",
        "    'Precio m3 (COP)',\n",
        "    'Dias_Emision_PagoOportuno',\n",
        "    'Dias_Lectura_Emision',\n",
        "    'Dias_PagoOportuno_PagoReal',\n",
        "]\n",
        "X = df_filtrado[features_for_pca].copy()\n",
        "\n",
        "print(f\"Shape antes de dropna: {X.shape}\")\n",
        "X.dropna(inplace=True) # Eliminar filas con NaNs en estas características\n",
        "print(f\"Shape después de dropna: {X.shape}\")\n",
        "\n",
        "print(\"\\nValores faltantes después de dropna:\")\n",
        "print(X.isnull().sum())"
      ],
      "id": "select_numeric_nan_code"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Escalado de Características\n",
        "PCA es sensible a la escala de las características. Por lo tanto, estandarizaremos los datos."
      ],
      "metadata": {
        "id": "scaling_markdown"
      },
      "id": "scaling_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaling_code"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"--- Datos escalados (primeras 5 filas) ---\")\n",
        "print(pd.DataFrame(X_scaled, columns=X.columns).head())"
      ],
      "id": "scaling_code"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Análisis de Componentes Principales (PCA)\n",
        "\n",
        "Reduciremos la dimensionalidad a 2 componentes principales para visualización."
      ],
      "metadata": {
        "id": "pca_markdown"
      },
      "id": "pca_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pca_code"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2) # Reducir a 2 componentes\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "df_pca = pd.DataFrame(data=X_pca, columns=['principal_component_1', 'principal_component_2'])\n",
        "\n",
        "print(\"--- Componentes Principales (primeras 5 filas) ---\")\n",
        "print(df_pca.head())\n",
        "\n",
        "print(f\"\\nVarianza explicada por cada componente: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Varianza explicada total (2 componentes): {pca.explained_variance_ratio_.sum():.2f}\")"
      ],
      "id": "pca_code"
    },
    {
      "cell_type": "code",
      "source": [
        "df_pca = df_pca.set_index(X.index)\n",
        "df_pca = df_pca.join(df_filtrado[['Mora']])\n",
        "\n",
        "print(\"\\n--- df_pca con la columna 'Mora' (primeras 5 filas) ---\")\n",
        "print(df_pca.head())\n",
        "\n",
        "print(\"\\n--- Información de df_pca ---\")\n",
        "df_pca.info()"
      ],
      "metadata": {
        "id": "wpU7pUQm0a0w"
      },
      "id": "wpU7pUQm0a0w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el gráfico de dispersión\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='principal_component_1',\n",
        "    y='principal_component_2',\n",
        "    hue='Mora',  # Colorear los puntos según el valor de 'Mora' (0 o 1)\n",
        "    data=df_pca,\n",
        "    palette='viridis', # Opcional: cambia la paleta de colores\n",
        "    alpha=0.6 # Opcional: ajusta la transparencia de los puntos\n",
        ")\n",
        "\n",
        "plt.title('Gráfico de Dispersión PCA: PCA1 vs PCA2 coloreado por Mora')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yo6sJYdB0m6E"
      },
      "id": "Yo6sJYdB0m6E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Definir las características (PCA componentes) y la variable objetivo\n",
        "X_model = df_pca[['principal_component_1', 'principal_component_2']]\n",
        "y_model = df_pca['Mora']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.3, random_state=42, stratify=y_model) # Stratify para mantener la proporción de 'Mora'\n",
        "\n",
        "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape[0]}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]}\")\n",
        "print(f\"Proporción de 'Mora' en el conjunto de entrenamiento: {y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Proporción de 'Mora' en el conjunto de prueba: {y_test.value_counts(normalize=True)}\")\n",
        "\n",
        "# Inicializar y entrenar el modelo de Regresión Logística\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(\"\\n--- Evaluación del Modelo ---\")\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nInforme de Clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nMatriz de Confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Opcional: Visualizar los límites de decisión (solo funciona bien para 2 componentes)\n",
        "# Puedes ejecutar esta parte para ver cómo el modelo separa las clases en el espacio PCA.\n",
        "\n",
        "# Crear una cuadrícula para plotear los límites de decisión\n",
        "x_min, x_max = X_model['principal_component_1'].min() - 0.5, X_model['principal_component_1'].max() + 0.5\n",
        "y_min, y_max = X_model['principal_component_2'].min() - 0.5, X_model['principal_component_2'].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Predecir la clase para cada punto en la cuadrícula\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plotear los límites de decisión y los puntos de datos\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis') # Colores de fondo para las regiones\n",
        "\n",
        "sns.scatterplot(\n",
        "    x='principal_component_1',\n",
        "    y='principal_component_2',\n",
        "    hue='Mora',\n",
        "    data=df_pca,\n",
        "    palette='viridis',\n",
        "    alpha=0.6,\n",
        "    edgecolor='k', # Añadir borde a los puntos para mejor visibilidad\n",
        "    s=50 # Ajustar tamaño de los puntos\n",
        ")\n",
        "\n",
        "plt.title('Límites de Decisión del Modelo de Clasificación con PCA')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "W07gwiGO0mwB"
      },
      "id": "W07gwiGO0mwB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar la Matriz de Confusión\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Mora (0)', 'Mora (1)'],\n",
        "            yticklabels=['No Mora (0)', 'Mora (1)'])\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()\n",
        "\n",
        "# Explicación de la Matriz de Confusión\n",
        "print(\"\\n--- Explicación de la Matriz de Confusión ---\")\n",
        "print(f\"La matriz de confusión muestra el rendimiento de nuestro modelo en el conjunto de prueba.\")\n",
        "print(f\"Las filas representan las clases reales (Valor Real), y las columnas representan las clases predichas (Predicción).\")\n",
        "print(f\"Tenemos 4 celdas principales:\")\n",
        "print(f\"  - Arriba a la izquierda (True Negatives, TN): {cm[0, 0]} casos donde el valor real era 0 (No Mora) y el modelo predijo 0 (No Mora).\")\n",
        "print(f\"  - Arriba a la derecha (False Positives, FP): {cm[0, 1]} casos donde el valor real era 0 (No Mora) pero el modelo predijo 1 (Mora). Estos son errores de 'Tipo I'.\")\n",
        "print(f\"  - Abajo a la izquierda (False Negatives, FN): {cm[1, 0]} casos donde el valor real era 1 (Mora) pero el modelo predijo 0 (No Mora). Estos son errores de 'Tipo II'.\")\n",
        "print(f\"  - Abajo a la derecha (True Positives, TP): {cm[1, 1]} casos donde el valor real era 1 (Mora) y el modelo predijo 1 (Mora).\")\n",
        "\n",
        "print(f\"\\nA partir de estos valores, se calculan métricas como:\")\n",
        "print(f\"  - Accuracy = (TN + TP) / Total de casos = ({cm[0, 0]} + {cm[1, 1]}) / {np.sum(cm):.0f} = {accuracy:.4f}\")\n",
        "print(f\"  - Precision (para la clase 1, Mora) = TP / (TP + FP) = {cm[1, 1]} / ({cm[1, 1]} + {cm[0, 1]}): Proporción de predicciones positivas (Mora) que fueron correctas.\")\n",
        "print(f\"  - Recall (Sensibilidad, para la clase 1, Mora) = TP / (TP + FN) = {cm[1, 1]} / ({cm[1, 1]} + {cm[1, 0]}): Proporción de casos reales positivos (Mora) que fueron identificados correctamente.\")\n",
        "print(f\"  - F1-Score (para la clase 1, Mora): Media armónica de Precision y Recall, útil cuando hay un desbalance de clases.\")\n",
        "print(f\"Estas métricas, especialmente Precision y Recall, nos dan una visión más detallada del rendimiento del modelo, particularmente en la identificación de casos de 'Mora', que puede ser la clase de interés principal.\")\n"
      ],
      "metadata": {
        "id": "uKYdJSF29ME8"
      },
      "id": "uKYdJSF29ME8",
      "execution_count": null,
      "outputs": []
    }
  ]
}